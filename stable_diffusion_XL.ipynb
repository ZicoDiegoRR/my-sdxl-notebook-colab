{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicoDiegoRR/my-sdxl-notebook-colab/blob/main/stable_diffusion_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hug-0Okf8t"
      },
      "source": [
        "###<font color=\"black\"> » <b><font color=\"red\">Installing Dependencies </b>💿</font> <font color=\"black\"> «\n",
        "#####ㅤRun this cell first before creating images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaGpmeILXSGl"
      },
      "outputs": [],
      "source": [
        "#@markdown <b>Run this first to install essential libraries!</b><br>\n",
        "#@markdown <small><p>Required to use the generator.\n",
        "from IPython.display import clear_output\n",
        "print(\"📥 | Connecting to Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "print(\"⚙️ | Downloading libraries...\")\n",
        "!pip install diffusers\n",
        "!pip install torch==2.4.0 torchvision torchaudio\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install opencv-python\n",
        "!pip install peft\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install compel\n",
        "!pip install controlnet-aux\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "print(\"📁 | All essential libraries have been downloaded.\")\n",
        "print(\"🖌 | You can start generating images now.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBZ305GvH7w"
      },
      "source": [
        "###<font color=\"black\"> » <b><font color=\"orange\">MultiControlNet<font color=\"black\">, <b><font color=\"magenta\"></b>IP-Adapter<font color=\"black\">, and <b><font color=\"Lime\">Inpainting</b> 🔧</font> <font color=\"black\"> «"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sdjCI-xvy5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import ControlNetModel, StableDiffusionXLPipeline, UniPCMultistepScheduler, StableDiffusionXLControlNetPipeline, AutoPipelineForInpainting, AutoencoderKL\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline as pipe\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import os.path\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Function to load the saved number from a file\n",
        "def load_number(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data['number']\n",
        "    except (FileNotFoundError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Function to save the number to a file\n",
        "def save_number(filename, number):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump({'number': number}, file)\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "def get_depth_map_display(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    return image\n",
        "# Main function to handle the logic\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    folder = \"/content/gdrive/MyDrive/\"\n",
        "    filename = os.path.join(folder, \"random_number.json\")\n",
        "    Freeze = False #@param {type:\"boolean\"}\n",
        "    saved_number = load_number(filename)\n",
        "    if not Freeze:\n",
        "        # Generate a new random number if Freeze is False\n",
        "        random_number = random.randint(1, 1000000000)\n",
        "        save_number(filename, random_number)\n",
        "        saved_number = load_number(filename)\n",
        "    else:\n",
        "        # Use the saved number if Freeze is True\n",
        "        if saved_number is not None:\n",
        "            saved_number = saved_number\n",
        "        else:\n",
        "            print(\"No saved number found. Please set Freeze to False to generate a new number first.\")\n",
        "\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Prompt 🖌</b><br>\n",
        "    #@markdown <small>What do you want to see from the image?</small><br>\n",
        "    #@markdown <small>Leave everything unchecked and set the IP-Adapter to \"None\" to generate using Text2Image pipeline.</small>\n",
        "    Prompt = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Currently supports HuggingFace and CivitAI models, but also works for any websites. For HuggingFace's model, provide only the repository's ID. For CivitAI and others, provide the direct link to the model. When you're getting 401 error from CivitAI's link, add your token below.</small>\n",
        "    Model = \"\" #@param {type:\"string\"}\n",
        "    Model_Format = \"Safe Tensor (.safetensors)\" #@param [\"Pickle Tensor (.ckpt)\", \"Safe Tensor (.safetensors)\"]\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Settings ⚙️</b><br>\n",
        "    #@markdown <small>Leave \"ip\" to use last pre-generated ControlNet image. Check the box and leave the link blank to use last pre-generated image. </small>\n",
        "    Width = 1024 #@param {type:\"slider\", min: 512, max:1536, step:64}\n",
        "    Height = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
        "    Steps = 28 #@param {type:\"number\"}\n",
        "    Scale = 6 #@param {type:\"slider\", min:1, max:12, step:0.1}\n",
        "    #@markdown <small> Only accepts one link for Variational Autoencoder. </small>\n",
        "    VAE_Link = \"\" #@param {type:\"string\"}\n",
        "    #@markdown ***\n",
        "    #@markdown <b> LoRA</b> 📁🖌️\n",
        "\n",
        "    #@markdown <small> You can use multiple direct links to the LoRAs using this format: </small>\n",
        "\n",
        "    #@markdown <font color=\"blue\"><u><small><small> <link/to/file.safetensors>, <link/to/file.bin>, ... </small></small></font></u>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space (or not) between comma and the next link. </small>\n",
        "    LoRA_URLs = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Same as the URLs. </small>\n",
        "    #@markdown <small> The first numbers represent the weight's scale for the first LoRA you just inputted. The logic also applies to every LoRA. </small>\n",
        "    Weight_Scale = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Passing CivitAI's token is optional, but required if you're getting 401 Unauthorized error. Do not share your CivitAI's API key to anyone else!</small>\n",
        "    Token = \"\" #@param {type:\"string\"}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>ControlNet</b> 🖼️🔧\n",
        "\n",
        "    #@markdown <small> Adjust the thresholds based on your needs. Put an image link to the <b> Canny_Link </b> for reference. </small>\n",
        "    minimum_canny_threshold = 230 #@param {type:\"slider\", min:10, max:500, step:5}\n",
        "    maximum_canny_threshold = 300 #@param {type:\"slider\", min:100, max:750, step:5}\n",
        "    Canny_Link = \"\" #@param {type:\"string\"}\n",
        "    Canny = False #@param {type:\"boolean\"}\n",
        "    Canny_Strength = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ————————————————————————\n",
        "    #@markdown <small> Put an image link to the <b> DepthMap_Link </b> for reference. </small>\n",
        "    DepthMap_Link = \"\" #@param {type:\"string\"}\n",
        "    Depth_Map = False #@param {type:\"boolean\"}\n",
        "    Depth_Strength = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ————————————————————————\n",
        "    #@markdown <small> Put an image link to the <b> OpenPose_Link </b> for reference. </small>\n",
        "    OpenPose_Link = \"\" #@param {type:\"string\"}\n",
        "    Open_Pose = False #@param {type:\"boolean\"}\n",
        "    Open_Pose_Strength = 0.7 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Inpainting</b> 🖼️🖌️\n",
        "\n",
        "    #@markdown <small> Set the <b>Inpainting_Image</b> to \"pre-generated text2image image\" for last generated Text2Image image, \"pre-generated controlnet image\" for last ControlNet's generated image, and \"previous inpainting image\" for last inpainted image. Or you can pick image online using its direct link.</small>\n",
        "    Inpainting_Image = \"\" #@param [\"pre-generated text2image image\", \"pre-generated controlnet image\", \"previous inpainting image\"] {allow-input:true}\n",
        "    Mask_Image = \"\" #@param {type:\"string\"}\n",
        "    Inpainting = False #@param {type:\"boolean\"}\n",
        "    Inpainting_Strength = 0.9 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>IP-Adapter</b> 🖼️📝\n",
        "\n",
        "    #@markdown <small> You can use multiple direct links to the images using this format: </small>\n",
        "\n",
        "    #@markdown <small><small><small> https://example1.com/.../file.jpg, https://example2.com/.../file.jpg, ... </small></small></small>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space between comma and the next link. </small>\n",
        "    IP_Adapter = \"None\" #@param [\"ip-adapter-plus_sdxl_vit-h.bin\", \"ip-adapter-plus-face_sdxl_vit-h.bin\", \"ip-adapter_sdxl_vit-h.bin\", \"None\"]\n",
        "    IP_Image_Link=\"\" #@param {type:\"string\"}\n",
        "    IP_Adapter_Strength= 1 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Negative Prompt ⛔</b><br>\n",
        "    #@markdown <small>What you <b>don't</b> want to see from the image? (optional)</small><br>\n",
        "    Negative_Prompt = \"\" #@param {type:\"string\"}\n",
        "    if Canny:\n",
        "        if Canny_Link == \"ip\":\n",
        "            Canny_link = \"/content/gen_2.jpg\"\n",
        "        elif not Canny_Link:\n",
        "            Canny_link = \"/content/gen.jpg\"\n",
        "        else:\n",
        "            Canny_link = Canny_Link\n",
        "    if Depth_Map:\n",
        "        if DepthMap_Link == \"ip\":\n",
        "            Depthmap_Link = \"/content/gen_2.jpg\"\n",
        "        elif not DepthMap_Link:\n",
        "            Depthmap_Link = \"/content/gen.jpg\"\n",
        "        else:\n",
        "            Depthmap_Link = DepthMap_Link\n",
        "    if Open_Pose:\n",
        "        if OpenPose_Link == \"ip\":\n",
        "            Openpose_Link = \"/content/gen_2.jpg\"\n",
        "        elif not OpenPose_Link:\n",
        "            Openpose_Link = \"/content/gen.jpg\"\n",
        "        else:\n",
        "            Openpose_Link = OpenPose_Link\n",
        "    if Inpainting:\n",
        "        if Canny or Depth_Map or Open_Pose:\n",
        "            raise TypeError(\"You checked both ControlNet and Inpainting, which will cause incompatibility issues during your run. As of now, there's no alternative way to merge StableDiffusionXLControlNetPipeline and StableDiffusionXLInpaintingPipeline without causing any issues. Perhaps you want to use only one of them?\")\n",
        "        if not Mask_Image:\n",
        "            raise ValueError(\"You checked Inpainting while you're leaving Mask_Image empty. Mask_Image is required for Inpainting!\")\n",
        "        else:\n",
        "            mask_image = load_image(Mask_Image).resize((1024, 1024))\n",
        "        if Inpainting_Image == \"pre-generated text2image image\":\n",
        "            inpaint_image = load_image(\"/content/gen.jpg\").resize((1024, 1024))\n",
        "        elif Inpainting_Image == \"pre-generated controlnet image\":\n",
        "            inpaint_image = load_image(\"/content/gen_2.jpg\").resize((1024, 1024))\n",
        "        elif Inpainting_Image == \"previous inpainting image\":\n",
        "            inpaint_image = load_image(\"/content/gen_3.jpg\")\n",
        "        else:\n",
        "            inpaint_image = load_image(Inpainting_Image)\n",
        "        display(make_image_grid([inpaint_image, mask_image], rows=1, cols=2))\n",
        "    if not IP_Image_Link and IP_Adapter != \"None\":\n",
        "        raise ValueError(f\"You selected {IP_Adapter}, but left the IP_Image_Link empty. Please change the IP_Adapter to None or add at least one image in IP_Image_Link!\")\n",
        "    controlnets = []\n",
        "    images = []\n",
        "    controlnets_scale = []\n",
        "    if Canny:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"🏞️ | Converting image with Canny Edge Detection...\")\n",
        "        c_img = load_image(Canny_link)\n",
        "        image_canny = np.array(c_img)\n",
        "        image_canny = cv2.Canny(image_canny, minimum_canny_threshold, maximum_canny_threshold)\n",
        "        image_canny = image_canny[:, :, None]\n",
        "        image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
        "        canny_image = Image.fromarray(image_canny)\n",
        "        print(\"✅ | Canny Edge Detection is complete.\")\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([c_img, canny_image], rows=1, cols=2))\n",
        "        images.append(canny_image.resize((1024, 1024)))\n",
        "        controlnets_scale.append(Canny_Strength)\n",
        "    if Depth_Map:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"🏞️ | Converting image with Depth Map...\")\n",
        "        image_depth = load_image(Depthmap_Link).resize((1024, 1024))\n",
        "        depth_estimator = pipe(\"depth-estimation\")\n",
        "        depth_map = get_depth_map(image_depth, depth_estimator).unsqueeze(0).half().to(\"cpu\")\n",
        "        images.append(depth_map)\n",
        "        depth_map_display = Image.fromarray(get_depth_map_display(image_depth, depth_estimator))\n",
        "        print(\"✅ | Depth Map is complete.\")\n",
        "        controlnets_scale.append(Depth_Strength)\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([image_depth, depth_map_display], rows=1, cols=2))\n",
        "    if Open_Pose:\n",
        "        openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\").to(\"cpu\")\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16).to(\"cuda\"))\n",
        "        print(\"🏞️ | Converting image with Open Pose...\")\n",
        "        image_openpose = load_image(Openpose_Link)\n",
        "        openpose_image = openpose(image_openpose)\n",
        "        images.append(openpose_image.resize((1024, 1024)))\n",
        "        print(\"✅ | Open Pose is done.\")\n",
        "        controlnets_scale.append(Open_Pose_Strength)\n",
        "        display(make_image_grid([image_openpose, openpose_image], rows=1, cols=2))\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        \"h94/IP-Adapter\",\n",
        "        subfolder=\"models/image_encoder\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    if VAE_Link:\n",
        "        if not os.path.exists(\"/content/VAE\"):\n",
        "            os.mkdir(\"VAE\")\n",
        "        os.system(f'cd /content/VAE; wget -O vae.safetensors \"{VAE_Link}\"')\n",
        "        os.system(f'cd /content/VAE; wget -N https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/vae/config.json')\n",
        "        vae = AutoencoderKL.from_single_file(\"/content/VAE/vae.safetensors\", torch_dtype=torch.float16, config=\"/content/VAE/config.json\", local_files_only=True)\n",
        "    if \"http\" not in Model:\n",
        "        if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        else:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    else:\n",
        "        if not os.path.exists(\"/content/Checkpoint\"):\n",
        "            os.mkdir(\"Checkpoint\")\n",
        "        if \".ckpt\" in Model_Format:\n",
        "            format = \".ckpt\"\n",
        "        elif \".safetensors\" in Model_Format:\n",
        "            format = \".safetensors\"\n",
        "        checkpoint_name = f\"checkpoint_model{format}\"\n",
        "        if Token and \"civitai.com\" in Model:\n",
        "            if \"?\" in Model or \"&\" in Model:\n",
        "                checkpoint_link = f\"{Model}&token={Token}\"\n",
        "            else:\n",
        "                checkpoint_link = f\"{Model}token={Token}\"\n",
        "        else:\n",
        "            checkpoint_link = Model\n",
        "        Model_folder = checkpoint_link.replace(\"/\", \"_\")\n",
        "        Model_path = f\"/content/Checkpoint/{Model_folder}/{checkpoint_name}\"\n",
        "        Model_path_folder = f\"/content/Checkpoint/{Model_folder}\"\n",
        "        if not os.path.exists(Model_path_folder):\n",
        "            os.mkdir(Model_path_folder)\n",
        "            !cd \"$Model_path_folder\"; wget -O \"$checkpoint_name\" \"$checkpoint_link\"\n",
        "        if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        else:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    pipeline.enable_xformers_memory_efficient_attention()\n",
        "    generator = torch.Generator(\"cpu\").manual_seed(saved_number)\n",
        "    pipeline.safety_checker = None\n",
        "    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True], truncate_long_prompts=False)\n",
        "    conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "    if LoRA_URLs:\n",
        "        lora_list = []\n",
        "        lora_path = []\n",
        "        lora_links = re.split(r\", h\", LoRA_URLs.replace(\", http\", \", hhttp\").replace(\",http\", \", hhttp\").replace(\", /c\", \", h/c\").replace(\",/c\", \", h/c\"))\n",
        "        if not os.path.exists(\"/content/LoRAs\"):\n",
        "            os.mkdir(\"LoRAs\")\n",
        "        if not Weight_Scale:\n",
        "            scales_string = [\"1\"] * len(lora_links)\n",
        "        elif Weight_Scale and len(re.split(r\",| ,\", Weight_Scale)) < len(lora_links):\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "            for j in range(len(lora_links) - len(scales_string)):\n",
        "                scales_string.append(\"1\")\n",
        "        else:\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "        scales = [float(num) for num in scales_string]\n",
        "\n",
        "        for i, link in enumerate(lora_links, start=1):\n",
        "            path_file = os.path.join(\"/content/LoRAs\", link.replace(\"/\", \"_\").replace(\".\", \"_\"))\n",
        "            if not os.path.exists(path_file) and \"http\" in link:\n",
        "                os.makedirs(path_file)\n",
        "            lora_name = link.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "            lora_file_name = f\"lora_{lora_name}.safetensors\"\n",
        "            if \"civitai.com\" in link and Token:\n",
        "                if \"&\" in link or \"?\" in link:\n",
        "                    civit_link = f\"{link}&token={Token}\"\n",
        "                else:\n",
        "                    civit_link = f\"{link}?token={Token}\"\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$civit_link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            elif link.startswith(\"http\"):\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            else:\n",
        "                if link.startswith(\"/content/gdrive/MyDrive\"):\n",
        "                    constructed_gdrive_link = link\n",
        "                else:\n",
        "                    constructed_gdrive_link = f\"/content/gdrive/MyDrive/{link}\"\n",
        "                link_from_gdrive = constructed_gdrive_link.split(\"/\")\n",
        "                lora_path.append(\"/\".join([word for word in link_from_gdrive if \".safetensors\" not in word]))\n",
        "                lora_list.append(link_from_gdrive[-1])\n",
        "        lora_weights = [word for word in lora_list if word.endswith(\".safetensors\")]\n",
        "        print(lora_weights)\n",
        "        lora_names = [word.replace(\".safetensors\", \"\") for word in lora_weights]\n",
        "        for p in range(len(lora_weights)):\n",
        "            pipeline.load_lora_weights(f\"{lora_path[p]}/{lora_weights[p]}\", adapter_name=lora_names[p])\n",
        "        pipeline.set_adapters(lora_names, adapter_weights=scales)\n",
        "    torch.cuda.empty_cache()\n",
        "    if IP_Adapter != \"None\":\n",
        "        adapter_image = []\n",
        "        simple_Url = IP_Image_Link.split(\", \")\n",
        "        for link in simple_Url:\n",
        "            adapter_image.append(load_image(link))\n",
        "        adapter_display = [element for element in adapter_image]\n",
        "        if len(adapter_display) % 3 == 0:\n",
        "            row = len(adapter_display)/3\n",
        "        else:\n",
        "            row = int(len(adapter_display)/3) + 1\n",
        "            for i in range(3*row - len(adapter_display)):\n",
        "                adapter_display.append(load_image(\"https://huggingface.co/IDK-ab0ut/BFIDIW9W29NFJSKAOAOXDOKERJ29W/resolve/main/placeholder.png\"))\n",
        "        if len(adapter_image) < 3:\n",
        "            column = len(adapter_image)\n",
        "        else:\n",
        "            column = 3\n",
        "        display(make_image_grid([element.resize((1024, 1024)) for element in adapter_display], rows=row, cols=column))\n",
        "        image_embeds = [adapter_image]\n",
        "        pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=IP_Adapter)\n",
        "        pipeline.set_ip_adapter_scale(IP_Adapter_Strength)\n",
        "    torch.cuda.empty_cache()\n",
        "    if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "        image_save = \"/content/gen.jpg\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "        else:\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "        image_save = \"/content/gen_3.jpg\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, image=inpaint_image, mask_image=mask_image, generator=generator,strength=Inpainting_Strength).images[0]\n",
        "        else:\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator, image=inpaint_image, mask_image=mask_image, strength=Inpainting_Strength).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    else:\n",
        "        image_save = \"/content/gen_2.jpg\"\n",
        "        if Inpainting:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=2,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=2,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                ).images[0]\n",
        "        else:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=2,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=2,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale\n",
        "                ).images[0]\n",
        "    print(image)\n",
        "    image.save(image_save)\n",
        "    display(image)\n",
        "    print(f\"Seed: {saved_number}\")\n",
        "    time.sleep(3)\n",
        "    os.kill(os.getpid(), 9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "t8hug-0Okf8t"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
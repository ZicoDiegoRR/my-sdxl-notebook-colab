{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicoDiegoRR/my-sdxl-notebook-colab/blob/main/stable_diffusion_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hug-0Okf8t"
      },
      "source": [
        "###<font color=\"black\"> » <b><font color=\"red\">Installing Dependencies </b>💿</font> <font color=\"black\"> «\n",
        "#####ㅤRun this cell first before creating images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaGpmeILXSGl",
        "outputId": "c21e7e9d-88a6-481a-ab8c-eaac97f1cb63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 | Connecting to Google Drive...\n",
            "Mounted at /content/gdrive\n",
            "⚙️ | Downloading libraries...\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Downloading diffusers-0.30.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.2\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting xformers\n",
            "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->xformers) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->xformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->xformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->xformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->xformers) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->xformers) (1.3.0)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.27.post2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.33.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.12.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Collecting compel\n",
            "  Downloading compel-2.0.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: diffusers>=0.11 in /usr/local/lib/python3.10/dist-packages (from compel) (0.30.2)\n",
            "Requirement already satisfied: pyparsing~=3.0 in /usr/local/lib/python3.10/dist-packages (from compel) (3.1.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from compel) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers~=4.25 in /usr/local/lib/python3.10/dist-packages (from compel) (4.44.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (0.24.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (0.4.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.11->compel) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.25->compel) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.25->compel) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.25->compel) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.25->compel) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->compel) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->compel) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->compel) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->compel) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->compel) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers>=0.11->compel) (3.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->compel) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.11->compel) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.11->compel) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.11->compel) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.11->compel) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->compel) (1.3.0)\n",
            "Downloading compel-2.0.3-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: compel\n",
            "Successfully installed compel-2.0.3\n",
            "Collecting controlnet-aux\n",
            "  Downloading controlnet_aux-0.0.9-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (2.4.0+cu121)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (8.4.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (0.24.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (1.13.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (3.15.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (9.4.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (0.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (0.19.0+cu121)\n",
            "Collecting timm<=0.6.7 (from controlnet-aux)\n",
            "  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from controlnet-aux) (0.23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->controlnet-aux) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->controlnet-aux) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->controlnet-aux) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->controlnet-aux) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->controlnet-aux) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->controlnet-aux) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->controlnet-aux) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->controlnet-aux) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->controlnet-aux) (4.66.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->controlnet-aux) (3.20.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux) (2024.8.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->controlnet-aux) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->controlnet-aux) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->controlnet-aux) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->controlnet-aux) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->controlnet-aux) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->controlnet-aux) (1.3.0)\n",
            "Downloading controlnet_aux-0.0.9-py3-none-any.whl (282 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.4/282.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm, controlnet-aux\n",
            "Successfully installed controlnet-aux-0.0.9 timm-0.6.7\n",
            "📁 | All essential libraries have been downloaded.\n",
            "🖌 | You can start generating images now.\n"
          ]
        }
      ],
      "source": [
        "#@markdown <b>Run this first to install essential libraries!</b><br>\n",
        "#@markdown <small><p>Required to use the generator.\n",
        "from IPython.display import clear_output\n",
        "print(\"📥 | Connecting to Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "print(\"⚙️ | Downloading libraries...\")\n",
        "!pip install diffusers\n",
        "!pip install torch==2.4.0 torchvision torchaudio\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install opencv-python\n",
        "!pip install peft\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install compel\n",
        "!pip install controlnet-aux\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "print(\"📁 | All essential libraries have been downloaded.\")\n",
        "print(\"🖌 | You can start generating images now.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBZ305GvH7w"
      },
      "source": [
        "###<font color=\"black\"> » <b><font color=\"orange\">MultiControlNet<font color=\"black\">, <b><font color=\"magenta\"></b>IP-Adapter<font color=\"black\">, and <b><font color=\"Lime\">Inpainting</b> 🔧</font> <font color=\"black\"> «"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sdjCI-xvy5",
        "outputId": "6dde47e2-b897-4445-b050-d578572fe4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dd83ffa3e10547d18e86d95d54aa3242",
            "72eafcea9fdd4f0ca9e9dfea9e996291",
            "3235fefa57504320a7c4c8e363bf7d5c",
            "1cde0974490a46d8a9f50d6be7ca1e7e",
            "e9a868c4294e4bba9a73a8afe8423a81",
            "34fbf4fcdc6c4dd59b14c5db5cbcf125",
            "0a82c2d6e97d42889f1192eeebcd90f5",
            "d4a4df0d36b4452888020935666d297f",
            "c3fd32016f4546778ed8392cc5e94927",
            "803b166f4e804ccd9c62cac2494458a7",
            "014443a9b4b743628093b33c83a9953d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd83ffa3e10547d18e86d95d54aa3242"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import ControlNetModel, AutoPipelineForText2Image, StableDiffusionXLPipeline, UniPCMultistepScheduler, StableDiffusionXLControlNetPipeline, AutoPipelineForInpainting, AutoencoderKL, DiffusionPipeline, StableDiffusionControlNetPipeline\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline as pipe\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import os.path\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Function to load the saved number from a file\n",
        "def load_number(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data['number']\n",
        "    except (FileNotFoundError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Function to save the number to a file\n",
        "def save_number(filename, number):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump({'number': number}, file)\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "def get_depth_map_display(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    return image\n",
        "# Main function to handle the logic\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    folder = \"/content/gdrive/MyDrive/\"\n",
        "    filename = os.path.join(folder, \"random_number.json\")\n",
        "    Freeze = False #@param {type:\"boolean\"}\n",
        "    saved_number = load_number(filename)\n",
        "    if not Freeze:\n",
        "        # Generate a new random number if Freeze is False\n",
        "        random_number = random.randint(1, 1000000000)\n",
        "        save_number(filename, random_number)\n",
        "        saved_number = load_number(filename)\n",
        "    else:\n",
        "        # Use the saved number if Freeze is True\n",
        "        if saved_number is not None:\n",
        "            saved_number = saved_number\n",
        "        else:\n",
        "            print(\"No saved number found. Please set Freeze to False to generate a new number first.\")\n",
        "\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Prompt 🖌</b><br>\n",
        "    #@markdown <small>What do you want to see from the image?</small><br>\n",
        "    #@markdown <small>Leave everything unchecked and set the IP-Adapter to \"None\" to generate using Text2Image pipeline.</small>\n",
        "    Prompt = \"umbreon\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Currently supports HuggingFace and CivitAI models, but also works for any websites. For HuggingFace's model, provide only the repository's ID. For CivitAI and others, provide the direct link to the model. When you're getting 401 error from CivitAI's link, add your token below.</small>\n",
        "    Model_Version = \"Stable Diffusion 1\" #@param [\"Stable Diffusion 1\", \"Stable Diffusion XL 1.0\"]\n",
        "    Model = \"IDK-ab0ut/Yiffymix_v43\" #@param {type:\"string\"}\n",
        "    Model_Format = \"Safe Tensor (.safetensors)\" #@param [\"Pickle Tensor (.ckpt)\", \"Safe Tensor (.safetensors)\"]\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Settings ⚙️</b><br>\n",
        "    #@markdown <small>Leave \"ip\" to use last pre-generated ControlNet image. Check the box and leave the link blank to use last pre-generated image. </small>\n",
        "    Width = 1024 #@param {type:\"slider\", min: 512, max:1536, step:64}\n",
        "    Height = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
        "    Steps = 28 #@param {type:\"number\"}\n",
        "    Scale = 8 #@param {type:\"slider\", min:1, max:12, step:0.1}\n",
        "    #@markdown <small> Only accepts one link for Variational Autoencoder. </small>\n",
        "    VAE_Link = \"\" #@param {type:\"string\"}\n",
        "    #@markdown ***\n",
        "    #@markdown <b> LoRA</b> 📁🖌️\n",
        "\n",
        "    #@markdown <small> You can use multiple direct links to the LoRAs using this format: </small>\n",
        "\n",
        "    #@markdown <font color=\"blue\"><u><small><small> <link/to/file.safetensors>, <link/to/file.bin>, ... </small></small></font></u>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space (or not) between comma and the next link. </small>\n",
        "    LoRA_URLs = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Same as the URLs. </small>\n",
        "    #@markdown <small> The first numbers represent the weight's scale for the first LoRA you just inputted. The logic also applies to every LoRA. </small>\n",
        "    Weight_Scale = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Passing CivitAI's token is optional, but required if you're getting 401 Unauthorized error. Do not share your CivitAI's API key to anyone else!</small>\n",
        "    Token = \"\" #@param {type:\"string\"}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>ControlNet</b> 🖼️🔧\n",
        "\n",
        "    #@markdown <small> Adjust the thresholds based on your needs. Put an image link to the <b> Canny_Link </b> for reference. </small>\n",
        "    minimum_canny_threshold = 230 #@param {type:\"slider\", min:10, max:500, step:5}\n",
        "    maximum_canny_threshold = 300 #@param {type:\"slider\", min:100, max:750, step:5}\n",
        "    Canny_Link = \"\" #@param {type:\"string\"}\n",
        "    Canny = False #@param {type:\"boolean\"}\n",
        "    Canny_Strength = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ————————————————————————\n",
        "    #@markdown <small> Put an image link to the <b> DepthMap_Link </b> for reference. </small>\n",
        "    DepthMap_Link = \"\" #@param {type:\"string\"}\n",
        "    Depth_Map = False #@param {type:\"boolean\"}\n",
        "    Depth_Strength = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ————————————————————————\n",
        "    #@markdown <small> Put an image link to the <b> OpenPose_Link </b> for reference. </small>\n",
        "    OpenPose_Link = \"\" #@param {type:\"string\"}\n",
        "    Open_Pose = False #@param {type:\"boolean\"}\n",
        "    Open_Pose_Strength = 0.7 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Inpainting</b> 🖼️🖌️\n",
        "\n",
        "    #@markdown <small> Set the <b>Inpainting_Image</b> to \"pre-generated text2image image\" for last generated Text2Image image, \"pre-generated controlnet image\" for last ControlNet's generated image, and \"previous inpainting image\" for last inpainted image. Or you can pick image online using its direct link.</small>\n",
        "    Inpainting_Image = \"\" #@param [\"pre-generated text2image image\", \"pre-generated controlnet image\", \"previous inpainting image\"] {allow-input:true}\n",
        "    Mask_Image = \"\" #@param {type:\"string\"}\n",
        "    Inpainting = False #@param {type:\"boolean\"}\n",
        "    Inpainting_Strength = 0.9 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>IP-Adapter</b> 🖼️📝\n",
        "\n",
        "    #@markdown <small> You can use multiple direct links to the images using this format: </small>\n",
        "\n",
        "    #@markdown <small><small><small> https://example1.com/.../file.jpg, https://example2.com/.../file.jpg, ... </small></small></small>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space between comma and the next link. </small>\n",
        "    IP_Adapter = \"ViT-G or ViT-H\" #@param [\"Plus\", \"Face\", \"ViT-G or ViT-H\", \"None\"]\n",
        "    IP_Image_Link=\"https://www.pokemon.com/static-assets/content-assets/cms2/img/pokedex/full/197.png\" #@param {type:\"string\"}\n",
        "    IP_Adapter_Strength= 1 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Negative Prompt ⛔</b><br>\n",
        "    #@markdown <small>What you <b>don't</b> want to see from the image? (optional)</small><br>\n",
        "    Negative_Prompt = \"bad quality, worst quality \" #@param {type:\"string\"}\n",
        "    if Canny:\n",
        "        if Canny_Link == \"ip\":\n",
        "            Canny_link = \"/content/gen_2.jpg\"\n",
        "        elif not Canny_Link:\n",
        "            Canny_link = \"/content/gen.jpg\"\n",
        "        else:\n",
        "            Canny_link = Canny_Link\n",
        "    if Depth_Map:\n",
        "        if DepthMap_Link == \"ip\":\n",
        "            Depthmap_Link = \"/content/gen_2.jpg\"\n",
        "        elif not DepthMap_Link:\n",
        "            Depthmap_Link = \"/content/gen.jpg\"\n",
        "        else:\n",
        "            Depthmap_Link = DepthMap_Link\n",
        "    if Open_Pose:\n",
        "        if OpenPose_Link == \"ip\":\n",
        "            Openpose_Link = \"/content/gen_2.jpg\"\n",
        "        elif not OpenPose_Link:\n",
        "            Openpose_Link = \"/content/gen.jpg\"\n",
        "        else:\n",
        "            Openpose_Link = OpenPose_Link\n",
        "    if Inpainting:\n",
        "        if Canny or Depth_Map or Open_Pose:\n",
        "            raise TypeError(\"You checked both ControlNet and Inpainting, which will cause incompatibility issues during your run. As of now, there's no alternative way to merge StableDiffusionXLControlNetPipeline and StableDiffusionXLInpaintingPipeline without causing any issues. Perhaps you want to use only one of them?\")\n",
        "        if not Mask_Image:\n",
        "            raise ValueError(\"You checked Inpainting while you're leaving Mask_Image empty. Mask_Image is required for Inpainting!\")\n",
        "        else:\n",
        "            mask_image = load_image(Mask_Image).resize((1024, 1024))\n",
        "        if Inpainting_Image == \"pre-generated text2image image\":\n",
        "            inpaint_image = load_image(\"/content/gen.jpg\").resize((1024, 1024))\n",
        "        elif Inpainting_Image == \"pre-generated controlnet image\":\n",
        "            inpaint_image = load_image(\"/content/gen_2.jpg\").resize((1024, 1024))\n",
        "        elif Inpainting_Image == \"previous inpainting image\":\n",
        "            inpaint_image = load_image(\"/content/gen_3.jpg\")\n",
        "        else:\n",
        "            inpaint_image = load_image(Inpainting_Image)\n",
        "        display(make_image_grid([inpaint_image, mask_image], rows=1, cols=2))\n",
        "    if not IP_Image_Link and IP_Adapter != \"None\":\n",
        "        raise ValueError(f\"You selected {IP_Adapter}, but left the IP_Image_Link empty. Please change the IP_Adapter to None or add at least one image in IP_Image_Link!\")\n",
        "    if Model_Version == \"Stable Diffusion 1\":\n",
        "        version = \"1\"\n",
        "        image_encoder_path = \"sdxl_models/image_encoder\"\n",
        "    elif Model_Version == \"Stable Diffusion XL 1.0\":\n",
        "        version = \"xl\"\n",
        "        image_encoder_path = \"models/image_encoder\"\n",
        "    if IP_Adapter != \"None\":\n",
        "        sdxl_adapter = [\"ip-adapter-plus_sdxl_vit-h.bin\", \"ip-adapter-plus-face_sdxl_vit-h.bin\", \"ip-adapter_sdxl_vit-h.bin\"]\n",
        "        sd_adapter = [\"ip-adapter-plus_sd15.bin\", \"ip-adapter-plus-face_sd15.bin\", \"ip-adapter_sd15_vit-G.bin\"]\n",
        "        ip_model = 0 if IP_Adapter == \"Plus\" else 1 if IP_Adapter == \"Face\" else 2\n",
        "        ip_adapter = sdxl_adapter[ip_model] if version == \"xl\" else sd_adapter[ip_model]\n",
        "        ip_folder = \"models\" if version == \"1\" else \"sdxl_models\"\n",
        "    controlnets = []\n",
        "    images = []\n",
        "    controlnets_scale = []\n",
        "    if Canny:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"🏞️ | Converting image with Canny Edge Detection...\")\n",
        "        c_img = load_image(Canny_link)\n",
        "        image_canny = np.array(c_img)\n",
        "        image_canny = cv2.Canny(image_canny, minimum_canny_threshold, maximum_canny_threshold)\n",
        "        image_canny = image_canny[:, :, None]\n",
        "        image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
        "        canny_image = Image.fromarray(image_canny)\n",
        "        print(\"✅ | Canny Edge Detection is complete.\")\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([c_img, canny_image], rows=1, cols=2))\n",
        "        images.append(canny_image.resize((1024, 1024)))\n",
        "        controlnets_scale.append(Canny_Strength)\n",
        "    if Depth_Map:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"🏞️ | Converting image with Depth Map...\")\n",
        "        image_depth = load_image(Depthmap_Link).resize((1024, 1024))\n",
        "        depth_estimator = pipe(\"depth-estimation\")\n",
        "        depth_map = get_depth_map(image_depth, depth_estimator).unsqueeze(0).half().to(\"cpu\")\n",
        "        images.append(depth_map)\n",
        "        depth_map_display = Image.fromarray(get_depth_map_display(image_depth, depth_estimator))\n",
        "        print(\"✅ | Depth Map is complete.\")\n",
        "        controlnets_scale.append(Depth_Strength)\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([image_depth, depth_map_display], rows=1, cols=2))\n",
        "    if Open_Pose:\n",
        "        openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\").to(\"cpu\")\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16).to(\"cuda\"))\n",
        "        print(\"🏞️ | Converting image with Open Pose...\")\n",
        "        image_openpose = load_image(Openpose_Link)\n",
        "        openpose_image = openpose(image_openpose)\n",
        "        images.append(openpose_image.resize((1024, 1024)))\n",
        "        print(\"✅ | Open Pose is done.\")\n",
        "        controlnets_scale.append(Open_Pose_Strength)\n",
        "        display(make_image_grid([image_openpose, openpose_image], rows=1, cols=2))\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        \"h94/IP-Adapter\",\n",
        "        subfolder=image_encoder_path,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    if VAE_Link:\n",
        "        if not os.path.exists(\"/content/VAE\"):\n",
        "            os.mkdir(\"VAE\")\n",
        "        if version == \"1\":\n",
        "            VAE_cfg = \"https://huggingface.co/pt-sk/stable-diffusion-1.5/resolve/main/vae/config.json\"\n",
        "        elif version == \"xl\":\n",
        "            VAE_cfg = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/vae/config.json\"\n",
        "        os.system(f'cd /content/VAE; wget -O vae.safetensors \"{VAE_Link}\"')\n",
        "        os.system(f'cd /content/VAE; wget -N \"{VAE_cfg}\"')\n",
        "        vae = AutoencoderKL.from_single_file(\"/content/VAE/vae.safetensors\", torch_dtype=torch.float16, config=\"/content/VAE/config.json\", local_files_only=True)\n",
        "    if \"http\" not in Model:\n",
        "        if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForText2Image.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        else:\n",
        "            if VAE_Link:\n",
        "                if version == \"1\":\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                elif version == \"xl\":\n",
        "                    pipeline = StableDiffusionControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, vae=vae, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                if version == \"1\":\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                elif version == \"xl\":\n",
        "                    pipeline = StableDiffusionControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    else:\n",
        "        if not os.path.exists(\"/content/Checkpoint\"):\n",
        "            os.mkdir(\"Checkpoint\")\n",
        "        if \".ckpt\" in Model_Format:\n",
        "            format = \".ckpt\"\n",
        "        elif \".safetensors\" in Model_Format:\n",
        "            format = \".safetensors\"\n",
        "        checkpoint_name = f\"checkpoint_model{format}\"\n",
        "        if Token and \"civitai.com\" in Model:\n",
        "            if \"?\" in Model or \"&\" in Model:\n",
        "                checkpoint_link = f\"{Model}&token={Token}\"\n",
        "            else:\n",
        "                checkpoint_link = f\"{Model}token={Token}\"\n",
        "        else:\n",
        "            checkpoint_link = Model\n",
        "        Model_folder = checkpoint_link.replace(\"/\", \"_\")\n",
        "        Model_path = f\"/content/Checkpoint/{Model_folder}/{checkpoint_name}\"\n",
        "        Model_path_folder = f\"/content/Checkpoint/{Model_folder}\"\n",
        "        if not os.path.exists(Model_path_folder):\n",
        "            os.mkdir(Model_path_folder)\n",
        "            !cd \"$Model_path_folder\"; wget -O \"$checkpoint_name\" \"$checkpoint_link\"\n",
        "        if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "            if VAE_Link:\n",
        "                if version == \"xl\":\n",
        "                    pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                elif version == \"1\":\n",
        "                    pipeline = StableDiffusionPipeline.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                if version == \"xl\":\n",
        "                    pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                elif version == \"1\":\n",
        "                    pipeline = DiffusionPipeline.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        else:\n",
        "            if VAE_Link:\n",
        "                if version == \"xl\":\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                if version == \"1\":\n",
        "                    pipeline = StableDiffusionControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                if version == \"xl\":\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                if version == \"1\":\n",
        "                    pipeline = StableDiffusionControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    pipeline.enable_xformers_memory_efficient_attention()\n",
        "    generator = torch.Generator(\"cpu\").manual_seed(saved_number)\n",
        "    pipeline.safety_checker = None\n",
        "    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    if version == \"xl\":\n",
        "        compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True], truncate_long_prompts=False)\n",
        "        conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "    elif version == \"1\":\n",
        "        compel = Compel(tokenizer=pipeline.tokenizer, text_encoder=pipeline.text_encoder, returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=False, truncate_long_prompts=False)\n",
        "        conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "        [conditioning,pooled]=compel.pad_conditioning_tensors_to_same_length([conditioning, pooled])\n",
        "    if LoRA_URLs:\n",
        "        lora_list = []\n",
        "        lora_path = []\n",
        "        lora_links = re.split(r\", h\", LoRA_URLs.replace(\", http\", \", hhttp\").replace(\",http\", \", hhttp\").replace(\", /c\", \", h/c\").replace(\",/c\", \", h/c\"))\n",
        "        if not os.path.exists(\"/content/LoRAs\"):\n",
        "            os.mkdir(\"LoRAs\")\n",
        "        if not Weight_Scale:\n",
        "            scales_string = [\"1\"] * len(lora_links)\n",
        "        elif Weight_Scale and len(re.split(r\",| ,\", Weight_Scale)) < len(lora_links):\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "            for j in range(len(lora_links) - len(scales_string)):\n",
        "                scales_string.append(\"1\")\n",
        "        else:\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "        scales = [float(num) for num in scales_string]\n",
        "\n",
        "        for i, link in enumerate(lora_links, start=1):\n",
        "            path_file = os.path.join(\"/content/LoRAs\", link.replace(\"/\", \"_\").replace(\".\", \"_\"))\n",
        "            if not os.path.exists(path_file) and \"http\" in link:\n",
        "                os.makedirs(path_file)\n",
        "            lora_name = link.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "            lora_file_name = f\"lora_{lora_name}.safetensors\"\n",
        "            if \"civitai.com\" in link and Token:\n",
        "                if \"&\" in link or \"?\" in link:\n",
        "                    civit_link = f\"{link}&token={Token}\"\n",
        "                else:\n",
        "                    civit_link = f\"{link}?token={Token}\"\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$civit_link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            elif link.startswith(\"http\"):\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            else:\n",
        "                if link.startswith(\"/content/gdrive/MyDrive\"):\n",
        "                    constructed_gdrive_link = link\n",
        "                else:\n",
        "                    constructed_gdrive_link = f\"/content/gdrive/MyDrive/{link}\"\n",
        "                link_from_gdrive = constructed_gdrive_link.split(\"/\")\n",
        "                lora_path.append(\"/\".join([word for word in link_from_gdrive if \".safetensors\" not in word]))\n",
        "                lora_list.append(link_from_gdrive[-1])\n",
        "        lora_weights = [word for word in lora_list if word.endswith(\".safetensors\")]\n",
        "        print(lora_weights)\n",
        "        lora_names = [word.replace(\".safetensors\", \"\") for word in lora_weights]\n",
        "        for p in range(len(lora_weights)):\n",
        "            pipeline.load_lora_weights(f\"{lora_path[p]}/{lora_weights[p]}\", adapter_name=lora_names[p])\n",
        "        pipeline.set_adapters(lora_names, adapter_weights=scales)\n",
        "    torch.cuda.empty_cache()\n",
        "    if IP_Adapter != \"None\":\n",
        "        adapter_image = []\n",
        "        simple_Url = IP_Image_Link.split(\", \")\n",
        "        for link in simple_Url:\n",
        "            adapter_image.append(load_image(link))\n",
        "        adapter_display = [element for element in adapter_image]\n",
        "        if len(adapter_display) % 3 == 0:\n",
        "            row = len(adapter_display)/3\n",
        "        else:\n",
        "            row = int(len(adapter_display)/3) + 1\n",
        "            for i in range(3*row - len(adapter_display)):\n",
        "                adapter_display.append(load_image(\"https://huggingface.co/IDK-ab0ut/BFIDIW9W29NFJSKAOAOXDOKERJ29W/resolve/main/placeholder.png\"))\n",
        "        display(make_image_grid([element.resize((1024, 1024)) for element in adapter_display], rows=row, cols=3))\n",
        "        image_embeds = [adapter_image]\n",
        "        pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=ip_folder, weight_name=ip_adapter)\n",
        "        pipeline.set_ip_adapter_scale(IP_Adapter_Strength)\n",
        "    torch.cuda.empty_cache()\n",
        "    if version == \"1\":\n",
        "        if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "            image_save = \"/content/gen.jpg\"\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "            else:\n",
        "                image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2].unsqueeze(0).repeat(1, 77, 768), negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "                pipeline.unload_ip_adapter()\n",
        "        elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "            image_save = \"/content/gen_3.jpg\"\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, image=inpaint_image, mask_image=mask_image, generator=generator,strength=Inpainting_Strength).images[0]\n",
        "            else:\n",
        "                image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator, image=inpaint_image, mask_image=mask_image, strength=Inpainting_Strength).images[0]\n",
        "                pipeline.unload_ip_adapter()\n",
        "        else:\n",
        "            image_save = \"/content/gen_2.jpg\"\n",
        "            if Inpainting:\n",
        "                if IP_Adapter == \"None\":\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=conditioning[0:1],\n",
        "                        pooled_prompt_embeds=pooled[0:1],\n",
        "                        negative_prompt_embeds=conditioning[1:2],\n",
        "                        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                        clip_skip=2,\n",
        "                        num_inference_steps=Steps,\n",
        "                        generator=generator,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale,\n",
        "                        guidance_scale=Scale,\n",
        "                    ).images[0]\n",
        "                else:\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=conditioning[0:1],\n",
        "                        pooled_prompt_embeds=pooled[0:1],\n",
        "                        negative_prompt_embeds=conditioning[1:2],\n",
        "                        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                        num_inference_steps=Steps,\n",
        "                        ip_adapter_image=image_embeds,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        guidance_scale=Scale,\n",
        "                        clip_skip=2,\n",
        "                        generator=generator,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale,\n",
        "                    ).images[0]\n",
        "            else:\n",
        "                if IP_Adapter == \"None\":\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=conditioning[0:1],\n",
        "                        pooled_prompt_embeds=pooled[0:1],\n",
        "                        negative_prompt_embeds=conditioning[1:2],\n",
        "                        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                        clip_skip=2,\n",
        "                        num_inference_steps=Steps,\n",
        "                        generator=generator,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale,\n",
        "                        guidance_scale=Scale,\n",
        "                    ).images[0]\n",
        "                else:\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=conditioning[0:1],\n",
        "                        pooled_prompt_embeds=pooled[0:1],\n",
        "                        negative_prompt_embeds=conditioning[1:2],\n",
        "                        negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                        num_inference_steps=Steps,\n",
        "                        ip_adapter_image=image_embeds,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        guidance_scale=Scale,\n",
        "                        clip_skip=2,\n",
        "                        generator=generator,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale\n",
        "                    ).images[0]\n",
        "    if version == \"xl\":\n",
        "        if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "            image_save = \"/content/gen.jpg\"\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds, negative_prompt=Negative_Prompt,num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "            else:\n",
        "                image = pipeline(prompt_embeds=prompt_embeds[0:1], pooled_prompt_embeds=pooled_prompt_embeds[0:1], negative_prompt=Negative_Prompt,num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "                pipeline.unload_ip_adapter()\n",
        "        elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "            image_save = \"/content/gen_3.jpg\"\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds, negative_prompt=Negative_Prompt, num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, image=inpaint_image, mask_image=mask_image, generator=generator,strength=Inpainting_Strength).images[0]\n",
        "            else:\n",
        "                image = pipeline(prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds, negative_prompt=Negative_Prompt, num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator, image=inpaint_image, mask_image=mask_image, strength=Inpainting_Strength).images[0]\n",
        "                pipeline.unload_ip_adapter()\n",
        "        else:\n",
        "            image_save = \"/content/gen_2.jpg\"\n",
        "            if Inpainting:\n",
        "                if IP_Adapter == \"None\":\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "                        negative_prompt=Negative_Prompt,\n",
        "                        clip_skip=2,\n",
        "                        num_inference_steps=Steps,\n",
        "                        generator=generator,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale,\n",
        "                        guidance_scale=Scale,\n",
        "                    ).images[0]\n",
        "                else:\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "                        negative_prompt=Negative_Prompt,\n",
        "                        num_inference_steps=Steps,\n",
        "                        ip_adapter_image=image_embeds,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        guidance_scale=Scale,\n",
        "                        clip_skip=2,\n",
        "                        generator=generator,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale,\n",
        "                    ).images[0]\n",
        "            else:\n",
        "                if IP_Adapter == \"None\":\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "                        negative_prompt=Negative_Prompt,\n",
        "                        clip_skip=2,\n",
        "                        num_inference_steps=Steps,\n",
        "                        generator=generator,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale,\n",
        "                        guidance_scale=Scale,\n",
        "                    ).images[0]\n",
        "                else:\n",
        "                    image = pipeline(\n",
        "                        prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "                        negative_prompt=Negative_Prompt,\n",
        "                        num_inference_steps=Steps,\n",
        "                        ip_adapter_image=image_embeds,\n",
        "                        width=Width,\n",
        "                        height=Height,\n",
        "                        guidance_scale=Scale,\n",
        "                        clip_skip=2,\n",
        "                        generator=generator,\n",
        "                        image=images,\n",
        "                        controlnet_conditioning_scale=controlnets_scale\n",
        "                    ).images[0]\n",
        "    print(image)\n",
        "    image.save(image_save)\n",
        "    display(image)\n",
        "    print(f\"Seed: {saved_number}\")\n",
        "    time.sleep(3)\n",
        "    os.kill(os.getpid(), 9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "t8hug-0Okf8t"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd83ffa3e10547d18e86d95d54aa3242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72eafcea9fdd4f0ca9e9dfea9e996291",
              "IPY_MODEL_3235fefa57504320a7c4c8e363bf7d5c",
              "IPY_MODEL_1cde0974490a46d8a9f50d6be7ca1e7e"
            ],
            "layout": "IPY_MODEL_e9a868c4294e4bba9a73a8afe8423a81"
          }
        },
        "72eafcea9fdd4f0ca9e9dfea9e996291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34fbf4fcdc6c4dd59b14c5db5cbcf125",
            "placeholder": "​",
            "style": "IPY_MODEL_0a82c2d6e97d42889f1192eeebcd90f5",
            "value": "Loading pipeline components...:  83%"
          }
        },
        "3235fefa57504320a7c4c8e363bf7d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4a4df0d36b4452888020935666d297f",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3fd32016f4546778ed8392cc5e94927",
            "value": 5
          }
        },
        "1cde0974490a46d8a9f50d6be7ca1e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803b166f4e804ccd9c62cac2494458a7",
            "placeholder": "​",
            "style": "IPY_MODEL_014443a9b4b743628093b33c83a9953d",
            "value": " 5/6 [00:03&lt;00:00,  1.24it/s]"
          }
        },
        "e9a868c4294e4bba9a73a8afe8423a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34fbf4fcdc6c4dd59b14c5db5cbcf125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a82c2d6e97d42889f1192eeebcd90f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a4df0d36b4452888020935666d297f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3fd32016f4546778ed8392cc5e94927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "803b166f4e804ccd9c62cac2494458a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014443a9b4b743628093b33c83a9953d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
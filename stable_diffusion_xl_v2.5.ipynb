{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicoDiegoRR/stable_diffusion_xl_colab_ui/blob/main/stable_diffusion_xl_v2.5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hug-0Okf8t"
      },
      "source": [
        "###<font color=\"black\"> » <b><font color=\"red\">Installing Dependencies </b>💿</font> <font color=\"black\"> «\n",
        "#####ㅤRun this cell first before creating images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GaGpmeILXSGl"
      },
      "outputs": [],
      "source": [
        "#@markdown <b>Run this first to install essential libraries!</b><br>\n",
        "#@markdown <small><p>Required to use the generator.\n",
        "from IPython.display import clear_output\n",
        "print(\"⚙️ | Downloading libraries...\")\n",
        "!pip install diffusers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install opencv-python\n",
        "!pip install peft\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install compel\n",
        "!pip install controlnet-aux\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "clear_output()\n",
        "print(\"📁 | All essential libraries have been downloaded.\")\n",
        "print(\"🖌 | You can start generating images now.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBZ305GvH7w"
      },
      "source": [
        "###<font color=\"black\"> »<b><font color=\"orange\">Running Stable Diffusion</b> 🔧</font> <font color=\"black\"> «"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sdjCI-xvy5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from PIL import Image as ImagePIL\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import ControlNetModel, StableDiffusionXLPipeline, StableDiffusionXLControlNetPipeline, AutoPipelineForInpainting, AutoencoderKL\n",
        "from diffusers import DDPMScheduler, DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler, KDPM2DiscreteScheduler, KDPM2AncestralDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, HeunDiscreteScheduler, LMSDiscreteScheduler, DEISMultistepScheduler, UniPCMultistepScheduler, DDIMScheduler, PNDMScheduler\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline as pipe\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "from google.colab import drive\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import re\n",
        "import os\n",
        "import subprocess\n",
        "import os.path\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "\n",
        "#@markdown <b>Run the cell to start!</b>\n",
        "\n",
        "#@markdown <small>Just run the cell and enjoy. (required to run the cell above first)</small>\n",
        "\n",
        "#@markdown <small>You can disable Google Drive by not permitting the notebook to access your Google Drive storage.</small>\n",
        "\n",
        "#@markdown <small>If the runtime got restarted, just run it again.</small>\n",
        "\n",
        "# Function to load parameters config\n",
        "def load_param(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            params = json.load(f)\n",
        "            print(f\"Found a config at {filename}.\")\n",
        "        return params\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Function to save the data to a json\n",
        "def save_last(filename, data, type):\n",
        "    try:\n",
        "        if os.path.exists(filename):\n",
        "            with open(filename, 'r') as file:\n",
        "                existing_data = json.load(file)\n",
        "        else:\n",
        "            existing_data = {}\n",
        "\n",
        "        if type == \"[Text-to-Image]\":\n",
        "            existing_data['text2img'] = data\n",
        "        elif type == \"[ControlNet]\":\n",
        "            existing_data['controlnet'] = data\n",
        "        elif type == \"[Inpainting]\":\n",
        "            existing_data['inpaint'] = data\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(existing_data, file, indent=4)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Function to load last-generated image\n",
        "def load_last(filename, type):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data.get(type, None)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return None\n",
        "\n",
        "# Function to load the saved data from a json\n",
        "def load_number(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data['saved']\n",
        "    except (FileNotFoundError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Function to save the data to a json\n",
        "def save_number(filename, data):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump({'saved': data}, file)\n",
        "\n",
        "#Function to save parameters config (had to make separate JSON def to avoid confusion)\n",
        "def save_param(path, data):\n",
        "    with open(path, 'w') as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "# Function to convert image into depth map\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "\n",
        "# Only for display in output, nothing crazy\n",
        "def get_depth_map_display(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    return image\n",
        "\n",
        "# Function to restart the runtime to free up some of the VRAM if there's a change in model or the pipeline\n",
        "def restart(new, old):\n",
        "    print(f\"New model is found. Your previous one ({old}) is different than your new one ({new}).\")\n",
        "    print(\"Restarting the runtime is necessary to load the new one.\")\n",
        "    time.sleep(2)\n",
        "    print(\"Restarting the runtime...\")\n",
        "    time.sleep(0.5)\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "# Loading the saved config for the IPyWidgets\n",
        "try:\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "except Exception as e:\n",
        "    print(\"Excluding Google Drive storage...\")\n",
        "    time.sleep(1.5)\n",
        "config_path_drive = os.path.join(\"/content/gdrive/MyDrive\", \"parameters.json\")\n",
        "config_path = os.path.join(\"/content\", \"parameters.json\")\n",
        "cfg_ver = load_param(config_path_drive)\n",
        "cfg = load_param(config_path_drive) if cfg_ver else load_param(config_path)\n",
        "if os.path.exists(\"/content/gdrive/MyDrive\"):\n",
        "    base_path = \"/content/gdrive/MyDrive\"\n",
        "    Save_and_Connect_To_GDrive = True\n",
        "else:\n",
        "    base_path = \"/content\"\n",
        "    Save_and_Connect_To_GDrive = False\n",
        "if not cfg:\n",
        "    print(\"No saved config found. Defaulting...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# IPyWidgets⬇️\n",
        "# ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
        "\n",
        "# Prompt Section\n",
        "prompt_widget = widgets.Text(value=cfg[0] if cfg else \"\", description=\"Prompt\", placeholder=\"Enter your prompt here\")\n",
        "model_widget = widgets.Text(value=cfg[1] if cfg else \"\", description=\"Model\", placeholder=\"HF's repository or direct URL\")\n",
        "model_format_widget = widgets.Dropdown(\n",
        "    options=[\"Pickle Tensor (.ckpt)\", \"Safe Tensor (.safetensors)\"],\n",
        "    value=cfg[2] if cfg else \"Safe Tensor (.safetensors)\",\n",
        "    description=\"Model Format\",\n",
        ")\n",
        "negative_prompt_widget = widgets.Text(value=cfg[3] if cfg else \"\", description=\"Negative Prompt\", placeholder=\"What you don't want to see?\")\n",
        "token_widget = widgets.Text(description=\"CivitAI Token\", placeholder=\"Avoid 401 error from CivitAI\")\n",
        "freeze_widget = widgets.Checkbox(description=\"Use the same seed\", value=cfg[31] if cfg else False)\n",
        "general_settings = widgets.VBox([widgets.HTML(value=\"<b>Image Generation Prompt🖌️</b>\"),\n",
        "    prompt_widget,\n",
        "    model_widget,\n",
        "    model_format_widget,\n",
        "    negative_prompt_widget,\n",
        "    freeze_widget,\n",
        "    token_widget,\n",
        "    widgets.HTML(value=\"For safety reason, your token <b>won't be saved</b>.\")\n",
        "])\n",
        "\n",
        "# Image Generation Settings\n",
        "width_slider = widgets.IntSlider(min=512, max=1536, step=64, value=cfg[4] if cfg else 1024, description=\"Width\")\n",
        "height_slider = widgets.IntSlider(min=512, max=1536, step=64, value=cfg[5] if cfg else 1024, description=\"Height\")\n",
        "steps_slider = widgets.IntText(value=cfg[7] if cfg else 12, description=\"Steps\")\n",
        "scale_slider = widgets.FloatSlider(min=1, max=12, step=0.1, value=cfg[8] if cfg else 6, description=\"Scale\")\n",
        "vae_link_widget = widgets.Text(value=cfg[9] if cfg else \"\", description=\"VAE Link\", placeholder=\"VAE model link\")\n",
        "vae_config = widgets.Text(value=cfg[36] if cfg else \"\", description=\"VAE Config Link\", placeholder=\"VAE config link\")\n",
        "clip_skip_slider = widgets.IntSlider(min=0, max=12, step=1, value=cfg[10] if cfg else 2, description=\"Clip Skip\")\n",
        "image_settings = widgets.VBox([\n",
        "    width_slider,\n",
        "    height_slider,\n",
        "    steps_slider,\n",
        "    scale_slider,\n",
        "    vae_link_widget,\n",
        "    vae_config,\n",
        "    clip_skip_slider\n",
        "])\n",
        "\n",
        "# Scheduler Section\n",
        "scheduler_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"Default (defaulting to the model)\", \"DPM++ 2M\", \"DPM++ 2M SDE\",\n",
        "        \"DPM++ SDE\", \"DPM2\", \"DDPM\",\n",
        "        \"DPM2 a\", \"DDIM\", \"PNDM\", \"Euler\", \"Euler a\", \"Heun\", \"LMS\",\n",
        "        \"DEISMultistep\", \"UniPCMultistep\"\n",
        "    ],\n",
        "    value=cfg[6] if cfg else \"Default (defaulting to the model)\",\n",
        "    description=\"Scheduler\",\n",
        ")\n",
        "karras_bool = widgets.Checkbox(value=cfg[32] if cfg else False, description=\"Enable Karras\")\n",
        "vpred_bool = widgets.Checkbox(value=cfg[33] if cfg else False, description=\"Enable V-prediction\")\n",
        "sgmuniform_bool = widgets.Checkbox(value=cfg[34] if cfg else False, description=\"Enable SGMUniform\")\n",
        "res_betas_zero_snr = widgets.Checkbox(value=cfg[35] if cfg else False, description=\"Rescale beta zero SNR\")\n",
        "scheduler_settings = widgets.VBox([\n",
        "    scheduler_dropdown,\n",
        "    karras_bool,\n",
        "    vpred_bool,\n",
        "    sgmuniform_bool,\n",
        "    res_betas_zero_snr,\n",
        "    widgets.HTML(value=\"Rescaling the betas to have zero terminal SNR helps to achieve vibrant color, but not necessary.\")\n",
        "])\n",
        "\n",
        "# LoRA Section\n",
        "def lora_click(link, scale): # Function to add widgets after clicking the plus button\n",
        "  lora_url_input = widgets.Text(value=link, placeholder=\"Input the link here\", description=\"Direct URL\")\n",
        "  lora_scale_input = widgets.FloatSlider(value=scale, min=-5, max=5, step=0.1, description=\"Weight Scale\")\n",
        "  lora_remove_button = widgets.Button(description=\"X\", button_style='danger', layout=widgets.Layout(width='30px', height='30px'))\n",
        "\n",
        "  lora_nested_vbox.children += (lora_url_input, lora_scale_input, lora_remove_button,)\n",
        "  lora_remove_button.on_click(lambda b: lora_remover(list(lora_nested_vbox.children).index(lora_remove_button) - 2, list(lora_nested_vbox.children).index(lora_remove_button) - 1, list(lora_nested_vbox.children).index(lora_remove_button)))\n",
        "  lora_settings.children = [lora_add, lora_nested_vbox]\n",
        "\n",
        "def lora_reader(): # Function to process every value from the widgets into two strings to be fed into the main logic (I'm too lazy to change the main code)\n",
        "  collected_lora_urls = \"\"\n",
        "  collected_lora_scales = \"\"\n",
        "  for i in range(len(lora_nested_vbox.children)):\n",
        "    if i % 3 == 0:\n",
        "      if lora_nested_vbox.children[i].value != \"\":\n",
        "        collected_lora_urls += (lora_nested_vbox.children[i].value + \",\")\n",
        "    elif i % 3 == 1:\n",
        "      if lora_nested_vbox.children[i - 1].value != \"\":\n",
        "        collected_lora_scales += (str(lora_nested_vbox.children[i].value) + \",\")\n",
        "  return collected_lora_urls, collected_lora_scales\n",
        "\n",
        "def lora_remover(link, scale, remove_button): # Function to remove lora (only the widgets, not the actual file)\n",
        "  lora_nested_list = list(lora_nested_vbox.children)\n",
        "  lora_nested_list.pop(remove_button)\n",
        "  lora_nested_list.pop(scale)\n",
        "  lora_nested_list.pop(link)\n",
        "  lora_nested_vbox.children = tuple(lora_nested_list)\n",
        "\n",
        "def lora_reader_upon_starting(): # Function to add widgets based on pre-existing URLs from the saved parameter\n",
        "  lora_links = [word for word in re.split(r\"\\s*,\\s*\", lora_urls_widget.value) if word]\n",
        "  lora_scales = [float(word) for word in re.split(r\"\\s*,\\s*\", weight_scale_widget.value) if word]\n",
        "  for i in range(len(lora_links)):\n",
        "    lora_click(lora_links[i], lora_scales[i])\n",
        "\n",
        "lora_urls_widget = widgets.Text(value=cfg[11] if cfg else \"\")\n",
        "weight_scale_widget = widgets.Text(value=cfg[12] if cfg else \"\")\n",
        "\n",
        "lora_add = widgets.Button(description=\"+\", button_style='success', layout=widgets.Layout(width='30px', height='30px'))\n",
        "lora_nested_vbox = widgets.VBox()\n",
        "lora_settings = widgets.VBox([lora_add])\n",
        "\n",
        "lora_add.on_click(lambda b: lora_click(\"\", 1.0))\n",
        "\n",
        "# ControlNet Section\n",
        "controlnet_dropdown_choice = [\"Link\", \"Upload\", \"Last Generated Text2Img\", \"Last Generated ControlNet\", \"Last Generated Inpainting\"]\n",
        "def controlnet_dropdown_handler(type, value): # Function to change the image reference based on the selected option in the dropdown\n",
        "  controlnet_url_widgets_list = [canny_link_widget, depth_map_link_widget, openpose_link_widget]\n",
        "  controlnet_upload_widgets_list = [canny_upload, depth_upload, openpose_upload]\n",
        "\n",
        "  controlnet_type = 0 if type == \"canny\" else 1 if type == \"depth\" else 2\n",
        "  controlnet_children = list(canny_settings.children) if controlnet_type == 0 else list(depth_settings.children) if controlnet_type == 1 else list(openpose_settings.children)\n",
        "  if value == \"Link\":\n",
        "    if controlnet_upload_widgets_list[controlnet_type] in controlnet_children:\n",
        "      controlnet_children.pop(2)\n",
        "    controlnet_children.insert(2, controlnet_url_widgets_list[controlnet_type])\n",
        "  elif value == \"Upload\":\n",
        "    if controlnet_url_widgets_list[controlnet_type] in controlnet_children:\n",
        "      controlnet_children.pop(2)\n",
        "    controlnet_children.insert(2, controlnet_upload_widgets_list[controlnet_type])\n",
        "  else:\n",
        "    if controlnet_url_widgets_list[controlnet_type] in controlnet_children:\n",
        "      controlnet_children.pop(2)\n",
        "    for i in range(len(controlnet_url_widgets_list)):\n",
        "      if controlnet_url_widgets_list[i] in controlnet_children:\n",
        "        controlnet_children.remove(controlnet_url_widgets_list[i])\n",
        "      if controlnet_upload_widgets_list[i] in controlnet_children:\n",
        "        controlnet_children.remove(controlnet_upload_widgets_list[i])\n",
        "    if controlnet_type == 0:\n",
        "      canny_link_widget.value = \"\" if value == \"Last Generated Text2Img\" else \"controlnet\" if value == \"Last Generated ControlNet\" else \"inpaint\"\n",
        "    elif controlnet_type == 1:\n",
        "      depth_map_link_widget.value = \"\" if value == \"Last Generated Text2Img\" else \"controlnet\" if value == \"Last Generated ControlNet\" else \"inpaint\"\n",
        "    else:\n",
        "      openpose_link_widget.value = \"\" if value == \"Last Generated Text2Img\" else \"controlnet\" if value == \"Last Generated ControlNet\" else \"inpaint\"\n",
        "\n",
        "  if controlnet_type == 0:\n",
        "    canny_settings.children = tuple(controlnet_children)\n",
        "  elif controlnet_type == 1:\n",
        "    depth_settings.children = tuple(controlnet_children)\n",
        "  else:\n",
        "    openpose_settings.children = tuple(controlnet_children)\n",
        "\n",
        "def canny_popup(change): # Function to display canny settings if true\n",
        "  if change[\"new\"]:\n",
        "    canny_settings.children = [canny_toggle, canny_dropdown, canny_min_slider, canny_max_slider, canny_strength_slider]\n",
        "  else:\n",
        "    canny_settings.children = [canny_toggle]\n",
        "\n",
        "def canny_dropdown_handler(change): # Function to attach the canny dropdown to the controlnet dropdown handler\n",
        "  controlnet_dropdown_handler(\"canny\", change[\"new\"])\n",
        "\n",
        "def canny_upload_handler(change): # Function to load the path of the uploaded image to the image link\n",
        "  if not os.path.exists(\"/content/canny/\"):\n",
        "    os.mkdir(\"/content/canny/\")\n",
        "  with open(\"/content/canny/temp.png\", \"wb\") as up:\n",
        "    up.write(canny_upload.content)\n",
        "  canny_link_widget.value = \"/content/canny/temp.png\"\n",
        "\n",
        "canny_upload = widgets.FileUpload(accept=\"image/*\", multiple=False)\n",
        "canny_link_widget = widgets.Text(value=cfg[15] if cfg and (not cfg[15].startswith(\"/content/canny/\") or os.path.exists(\"/content/canny/\")) else \"\", description=\"Canny Link\", placeholder=\"Image link\")\n",
        "\n",
        "canny_dropdown = widgets.Dropdown(options=controlnet_dropdown_choice, value=controlnet_dropdown_choice[2] if canny_link_widget.value == \"\" else controlnet_dropdown_choice[3] if canny_link_widget.value == \"controlnet\" else controlnet_dropdown_choice[4] if canny_link_widget.value == \"inpaint\" else controlnet_dropdown_choice[0], disabled=False, description=\"Reference Image\")\n",
        "canny_min_slider = widgets.IntSlider(min=10, max=500, step=5, value=cfg[13] if cfg else 100, description=\"Min Threshold\")\n",
        "canny_max_slider = widgets.IntSlider(min=100, max=750, step=5, value=cfg[14] if cfg else 240, description=\"Max Threshold\")\n",
        "canny_toggle = widgets.Checkbox(value=cfg[16] if cfg else False, description=\"Enable Canny\")\n",
        "canny_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[17] if cfg else 0.7, description=\"Canny Strength\")\n",
        "canny_settings = widgets.VBox([canny_toggle])\n",
        "\n",
        "canny_popup({\"new\": canny_toggle.value})\n",
        "canny_upload.observe(canny_upload_handler, names=\"value\")\n",
        "\n",
        "def depthmap_popup(change): # Function to display depth map settings if true\n",
        "  if change[\"new\"]:\n",
        "    depth_settings.children = [depth_map_toggle, depthmap_dropdown, depth_strength_slider]\n",
        "  else:\n",
        "    depth_settings.children = [depth_map_toggle]\n",
        "\n",
        "def depthmap_dropdown_handler(change): # Function to attach the canny dropdown to the controlnet dropdown handler\n",
        "  controlnet_dropdown_handler(\"depth\", change[\"new\"])\n",
        "\n",
        "def depthmap_upload_handler(change): # Function to load the path of the uploaded image to the image link\n",
        "  if not os.path.exists(\"/content/depthmap/\"):\n",
        "    os.mkdir(\"/content/depthmap/\")\n",
        "  with open(\"/content/depthmap/temp.png\", \"wb\") as up:\n",
        "    up.write(depth_upload.content)\n",
        "  depth_map_link_widget.value = \"/content/depthmap/temp.png\"\n",
        "\n",
        "depth_upload = widgets.FileUpload(accept=\"image/*\", multiple=False)\n",
        "depth_map_link_widget = widgets.Text(value=cfg[18] if cfg and (not cfg[18].startswith(\"/content/depthmap/\") or os.path.exists(\"/content/depthmap/\")) else \"\", description=\"DepthMap Link\", placeholder=\"Image link\")\n",
        "\n",
        "depthmap_dropdown = widgets.Dropdown(options=controlnet_dropdown_choice, value=controlnet_dropdown_choice[2] if depth_map_link_widget.value == \"\" else controlnet_dropdown_choice[3] if depth_map_link_widget.value == \"controlnet\" else controlnet_dropdown_choice[4] if depth_map_link_widget.value == \"inpaint\" else controlnet_dropdown_choice[0], disabled=False, description=\"Reference Image\")\n",
        "depth_map_toggle = widgets.Checkbox(value=cfg[19] if cfg else False, description=\"Enable Depth Map\")\n",
        "depth_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[20] if cfg else 0.7, description=\"Depth Strength\")\n",
        "depth_settings = widgets.VBox([depth_map_toggle])\n",
        "\n",
        "depthmap_popup({\"new\": depth_map_toggle.value})\n",
        "depth_upload.observe(depthmap_upload_handler, names=\"value\")\n",
        "\n",
        "def openpose_popup(change):  # Function to display openpose settings if true\n",
        "  if change[\"new\"]:\n",
        "    openpose_settings.children = [openpose_toggle, openpose_dropdown, openpose_strength_slider]\n",
        "  else:\n",
        "    openpose_settings.children = [openpose_toggle]\n",
        "\n",
        "def openpose_dropdown_handler(change): # Function to attach the canny dropdown to the controlnet dropdown handler\n",
        "  controlnet_dropdown_handler(\"openpose\", change[\"new\"])\n",
        "\n",
        "def openpose_upload_handler(change): # Function to load the path of the uploaded image to the image link\n",
        "  print(openpose_upload.value)\n",
        "  if not os.path.exists(\"/content/openpose/\"):\n",
        "    os.mkdir(\"/content/openpose/\")\n",
        "  for file_info in openpose_upload.value.items():\n",
        "    openpose_uploaded_image = file_info[1][\"content\"]\n",
        "    with open(\"/content/openpose/temp.png\", \"wb\") as up:\n",
        "      up.write(openpose_uploaded_image)\n",
        "  openpose_link_widget.value = \"/content/openpose/temp.png\"\n",
        "\n",
        "openpose_upload = widgets.FileUpload(accept=\"image/*\", multiple=False)\n",
        "openpose_link_widget = widgets.Text(value=cfg[21] if cfg and (not cfg[21].startswith(\"/content/openpose/\") or os.path.exists(\"/content/openpose/\")) else \"\", description=\"OpenPose Link\", placeholder=\"Image link\")\n",
        "\n",
        "openpose_dropdown = widgets.Dropdown(options=controlnet_dropdown_choice, value=controlnet_dropdown_choice[2] if openpose_link_widget.value == \"\" else controlnet_dropdown_choice[3] if openpose_link_widget.value == \"controlnet\" else controlnet_dropdown_choice[4] if openpose_link_widget.value == \"inpaint\" else controlnet_dropdown_choice[0], disabled=False, description=\"Reference Image\")\n",
        "openpose_toggle = widgets.Checkbox(value=cfg[22] if cfg else False, description=\"Enable OpenPose\")\n",
        "openpose_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[23] if cfg else 0.7, description=\"OpenPose Strength\")\n",
        "openpose_settings = widgets.VBox([openpose_toggle])\n",
        "\n",
        "openpose_popup({\"new\": openpose_toggle.value})\n",
        "openpose_upload.observe(openpose_upload_handler, names=\"value\")\n",
        "\n",
        "canny_dropdown_handler({\"new\": canny_dropdown.value})\n",
        "depthmap_dropdown_handler({\"new\": depthmap_dropdown.value})\n",
        "openpose_dropdown_handler({\"new\": openpose_dropdown.value})\n",
        "\n",
        "canny_dropdown.observe(canny_dropdown_handler, names=\"value\")\n",
        "depthmap_dropdown.observe(depthmap_dropdown_handler, names=\"value\")\n",
        "openpose_dropdown.observe(openpose_dropdown_handler, names=\"value\")\n",
        "\n",
        "canny_toggle.observe(canny_popup, names=\"value\")\n",
        "depth_map_toggle.observe(depthmap_popup, names=\"value\")\n",
        "openpose_toggle.observe(openpose_popup, names=\"value\")\n",
        "\n",
        "controlnet_settings = widgets.Accordion([canny_settings, depth_settings, openpose_settings])\n",
        "controlnet_settings.set_title(0, \"Canny📝\")\n",
        "controlnet_settings.set_title(1, \"Depth Map🏔️\")\n",
        "controlnet_settings.set_title(2, \"Open Pose🕺🏻\")\n",
        "\n",
        "# Inpainting Section\n",
        "inpainting_image_dropdown = widgets.Combobox(\n",
        "    options=[\n",
        "        \"pre-generated text2image image\",\n",
        "        \"pre-generated controlnet image\",\n",
        "        \"previous inpainting image\"\n",
        "    ],\n",
        "    value=cfg[24] if cfg else \"pre-generated text2image image\",\n",
        "    description=\"Inpainting Image\",\n",
        "    ensure_option=False\n",
        ")\n",
        "mask_image_widget = widgets.Text(value=cfg[25] if cfg else \"\", description=\"Mask Image\", placeholder=\"Image link\")\n",
        "inpainting_toggle = widgets.Checkbox(value=cfg[26] if cfg else False, description=\"Enable Inpainting\")\n",
        "inpainting_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[27] if cfg else 0.9, description=\"Inpainting Strength\")\n",
        "inpainting_settings = widgets.VBox([\n",
        "    inpainting_image_dropdown,\n",
        "    mask_image_widget,\n",
        "    inpainting_toggle,\n",
        "    inpainting_strength_slider\n",
        "])\n",
        "\n",
        "# IP-Adapter Section\n",
        "ip_adapter_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"ip-adapter-plus_sdxl_vit-h.bin\",\n",
        "        \"ip-adapter-plus-face_sdxl_vit-h.bin\",\n",
        "        \"ip-adapter_sdxl_vit-h.bin\",\n",
        "        \"None\"\n",
        "    ],\n",
        "    value=cfg[28] if cfg else \"None\",\n",
        "    description=\"IP-Adapter\",\n",
        ")\n",
        "ip_image_link_widget = widgets.Text(value=cfg[29] if cfg else \"\", description=\"IP Image Link\", placeholder=\"Image links separated by commas\")\n",
        "ip_adapter_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[30] if cfg else 0.8, description=\"Adapter Strength\")\n",
        "ip_settings = widgets.VBox([\n",
        "    ip_adapter_dropdown,\n",
        "    ip_image_link_widget,\n",
        "    ip_adapter_strength_slider\n",
        "])\n",
        "\n",
        "# Miscellaneous (generate button and freeze feature)\n",
        "submit_button_widget = widgets.Button(disabled=False, button_style='', description=\"Generate\")\n",
        "dont_spam = widgets.HTML(value=\"Please <b>don't spam</b> the generate button!\")\n",
        "keep_generating = widgets.HTML(value=\"You still can generate even though the cell is complete executing.\")\n",
        "submit_display = widgets.VBox([submit_button_widget, dont_spam, keep_generating])\n",
        "\n",
        "loaded_model = \"\"\n",
        "loaded_pipeline = \"\"\n",
        "\n",
        "# Miscellaneous (history)\n",
        "def history_button_handler(path): # Function to show and replace image from history upon clicking a button\n",
        "  history_image_widget.value = open(path, \"rb\").read()\n",
        "  history_image_modification_date.value = f\"Last modification time: {time.strftime('%B, %d %Y %H:%M:%S', time.localtime(os.path.getmtime(path)))}\"\n",
        "\n",
        "def grid(list): # Function to make a grid of buttons\n",
        "  list_grid = widgets.GridspecLayout(math.ceil(len(list)/10), 10) if list else widgets.HTML(value=\"Nothing in here currently.\")\n",
        "  if list:\n",
        "    for i in range(math.ceil(len(list)/10)):\n",
        "      for j in range(10):\n",
        "        k = (i*10 + j + 1)\n",
        "        list_grid[i, j] = widgets.Button(description=str(k), layout=widgets.Layout(height='auto', width='auto')) if k <= len(list) else widgets.Button(description=\"\", layout=widgets.Layout(height='auto', width='auto'))\n",
        "        path = list[k - 1] if k <= len(list) else \"\"\n",
        "        list_grid[i, j].on_click(lambda b, path=path: history_button_handler(path)) if k <= len(list) else None\n",
        "  return list_grid\n",
        "\n",
        "def history_display(): # Main logic for history\n",
        "  text2img_listdir = sorted([os.path.join(f\"{base_path}/Text2Img\", element) for element in os.listdir(f\"{base_path}/Text2Img\") if element.endswith(\".png\") and os.path.isfile(os.path.join(f\"{base_path}/Text2Img\", element))], key=os.path.getmtime, reverse=True) if os.path.exists(f\"{base_path}/Text2Img\") else []\n",
        "  controlnet_listdir = sorted([os.path.join(f\"{base_path}/ControlNet\", element) for element in os.listdir(f\"{base_path}/ControlNet\") if element.endswith(\".png\") and os.path.isfile(os.path.join(f\"{base_path}/ControlNet\", element))], key=os.path.getmtime, reverse=True) if os.path.exists(f\"{base_path}/ControlNet\") else []\n",
        "  inpainting_listdir = sorted([os.path.join(f\"{base_path}/Inpainting\", element) for element in os.listdir(f\"{base_path}/Inpainting\") if element.endswith(\".png\") and os.path.isfile(os.path.join(f\"{base_path}/Inpainting\", element))], key=os.path.getmtime, reverse=True) if os.path.exists(f\"{base_path}/Inpainting\") else []\n",
        "\n",
        "  text2img_list = grid(text2img_listdir)\n",
        "  controlnet_list = grid(controlnet_listdir)\n",
        "  inpainting_list = grid(inpainting_listdir)\n",
        "\n",
        "  history_accordion = widgets.Accordion(continuous_update = True)\n",
        "  history_image_modification_date = widgets.HTML()\n",
        "  history_image_widget = widgets.Image()\n",
        "\n",
        "  history_image_display_first = widgets.VBox([widgets.HTML(value=\"Image will show up here. (from the newest to the oldest)\"), history_image_widget, history_image_modification_date])\n",
        "  history_accordion.children = [text2img_list, controlnet_list, inpainting_list]\n",
        "\n",
        "  history_accordion.set_title(0, \"Text-to-Image History\")\n",
        "  history_accordion.set_title(1, \"ControlNet History\")\n",
        "  history_accordion.set_title(2, \"Inpainting History\")\n",
        "  history_display_vbox = widgets.VBox([history_accordion, history_image_display_first])\n",
        "  return text2img_list, controlnet_list, inpainting_list\n",
        "\n",
        "history_accordion = widgets.Accordion(continuous_update = True)\n",
        "history_image_modification_date = widgets.HTML()\n",
        "history_image_widget = widgets.Image()\n",
        "\n",
        "history_image_display_first = widgets.VBox([widgets.HTML(value=\"Image will show up here. (from the newest to the oldest)\"), history_image_widget, history_image_modification_date], continuous_update = True)\n",
        "text2img_list, controlnet_list, inpainting_list = history_display()\n",
        "history_accordion.children = [text2img_list, controlnet_list, inpainting_list]\n",
        "\n",
        "history_accordion.set_title(0, \"Text-to-Image History\")\n",
        "history_accordion.set_title(1, \"ControlNet History\")\n",
        "history_accordion.set_title(2, \"Inpainting History\")\n",
        "history_display_vbox = widgets.VBox([history_accordion, history_image_display_first], continuous_update = True)\n",
        "\n",
        "# Accordion, Tab, and  UI display grouping\n",
        "advanced_settings_ui = widgets.Accordion([image_settings, scheduler_settings, lora_settings, controlnet_settings, inpainting_settings, ip_settings])\n",
        "advanced_settings_titles = [\"Image Settings⚙️\", \"Scheduler Settings🖼️⚙️\", \"LoRA Settings📁🖌️\", \"ControlNet Settings 🖼️🔧\", \"Inpainting Settings🖼️🖌️\", \"IP-Adapter Settings 🖼️📝\"]\n",
        "for i, title in enumerate(advanced_settings_titles):\n",
        "    advanced_settings_ui.set_title(i, title)\n",
        "\n",
        "ui = widgets.Tab()\n",
        "ui.children = [general_settings, advanced_settings_ui, history_display_vbox]\n",
        "ui.set_title(0, \"General Settings\")\n",
        "ui.set_title(1, \"Advanced Settings\")\n",
        "ui.set_title(2, \"History\")\n",
        "# ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
        "\n",
        "# The IPyWidgets handler\n",
        "\n",
        "def widget(): # Function to display the ui and update the history\n",
        "    text2img_list, controlnet_list, inpainting_list = history_display()\n",
        "    history_accordion.children = [text2img_list, controlnet_list, inpainting_list]\n",
        "    history_display_vbox.children = [history_accordion, history_image_display_first]\n",
        "\n",
        "    clear_output()\n",
        "    submit_display.layout.display = \"inline-block\"\n",
        "    lora_add.layout.display = \"inline-block\"\n",
        "    display(ui, submit_display)\n",
        "\n",
        "def submit(_): # Function to trigger the main logic\n",
        "    submit_display.layout.display = \"none\"\n",
        "    lora_add.layout.display = \"none\"\n",
        "    lora_urls_widget.value, weight_scale_widget.value = lora_reader()\n",
        "    submit_button()\n",
        "\n",
        "# Main logic\n",
        "def submit_button():\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    Freeze = freeze_widget.value\n",
        "\n",
        "    # Handling Google Drive and seed\n",
        "    filename = os.path.join(base_path, \"random_number.json\")\n",
        "    saved_number = load_number(filename)\n",
        "    if not Freeze:\n",
        "        # Generate a new random number if Freeze is False\n",
        "        random_number = random.randint(1, 1000000000000)\n",
        "        save_number(filename, random_number)\n",
        "        saved_number = load_number(filename)\n",
        "    else:\n",
        "        # Use the saved number if Freeze is True\n",
        "        if saved_number is not None:\n",
        "            saved_number = saved_number\n",
        "        else:\n",
        "            print(\"No saved seed found. Generating new one...\")\n",
        "            random_number = random.randint(1, 1000000000000)\n",
        "            save_number(filename, random_number)\n",
        "            saved_number = load_number(filename)\n",
        "\n",
        "    # Handling user's input⬇️\n",
        "    # ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
        "    Prompt = prompt_widget.value\n",
        "    Model = model_widget.value\n",
        "    Model_Format = model_format_widget.value\n",
        "    Negative_Prompt = negative_prompt_widget.value\n",
        "\n",
        "    Width = width_slider.value\n",
        "    Height = height_slider.value\n",
        "    Steps = steps_slider.value\n",
        "    Scale = scale_slider.value\n",
        "    VAE_Link = vae_link_widget.value\n",
        "    VAE_Config = vae_config.value\n",
        "    Clip_Skip = clip_skip_slider.value\n",
        "\n",
        "    Scheduler = scheduler_dropdown.value\n",
        "    Karras = karras_bool.value\n",
        "    V_Prediction = vpred_bool.value\n",
        "    SGMUniform = sgmuniform_bool.value\n",
        "    Rescale_betas_to_zero_SNR = res_betas_zero_snr.value\n",
        "\n",
        "    LoRA_URLs = lora_urls_widget.value\n",
        "    Weight_Scale = weight_scale_widget.value\n",
        "    Token = token_widget.value\n",
        "\n",
        "    minimum_canny_threshold = canny_min_slider.value\n",
        "    maximum_canny_threshold = canny_max_slider.value\n",
        "    Canny_Link = canny_link_widget.value\n",
        "    Canny = canny_toggle.value\n",
        "    Canny_Strength = canny_strength_slider.value\n",
        "\n",
        "    DepthMap_Link = depth_map_link_widget.value\n",
        "    Depth_Map = depth_map_toggle.value\n",
        "    Depth_Strength = depth_strength_slider.value\n",
        "\n",
        "    OpenPose_Link = openpose_link_widget.value\n",
        "    Open_Pose = openpose_toggle.value\n",
        "    Open_Pose_Strength = openpose_strength_slider.value\n",
        "\n",
        "    Inpainting_Image = inpainting_image_dropdown.value\n",
        "    Mask_Image = mask_image_widget.value\n",
        "    Inpainting = inpainting_toggle.value\n",
        "    Inpainting_Strength = inpainting_strength_slider.value\n",
        "\n",
        "    IP_Adapter = ip_adapter_dropdown.value\n",
        "    IP_Image_Link = ip_image_link_widget.value\n",
        "    IP_Adapter_Strength = ip_adapter_strength_slider.value\n",
        "    # ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
        "\n",
        "    # Selecting image\n",
        "    last_generation_loading = os.path.join(base_path, \"last_generation.json\")\n",
        "    global pipeline_type\n",
        "    if Canny:\n",
        "        if Canny_Link == \"inpaint\":\n",
        "            Canny_link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif Canny_Link == \"controlnet\":\n",
        "            Canny_link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not Canny_Link:\n",
        "            Canny_link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Canny_link = Canny_Link\n",
        "        if Canny_link or os.path.exists(Canny_link):\n",
        "            pipeline_type = \"controlnet\"\n",
        "    else:\n",
        "        Canny_link = \"\"\n",
        "    if Depth_Map:\n",
        "        if DepthMap_Link == \"inpaint\":\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif DepthMap_Link == \"controlnet\":\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not DepthMap_Link:\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Depthmap_Link = DepthMap_Link\n",
        "        if Depthmap_Link or os.path.exists(Depthmap_Link):\n",
        "            pipeline_type = \"controlnet\"\n",
        "    else:\n",
        "        Depthmap_Link = \"\"\n",
        "    if Open_Pose:\n",
        "        if OpenPose_Link == \"inpaint\":\n",
        "            Openpose_Link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif OpenPose_Link == \"controlnet\":\n",
        "            Openpose_Link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not OpenPose_Link:\n",
        "            Openpose_Link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Openpose_Link = OpenPose_Link\n",
        "        if Openpose_Link or os.path.exists(Openpose_Link):\n",
        "            pipeline_type = \"controlnet\"\n",
        "    else:\n",
        "        Openpose_Link = \"\"\n",
        "    active_inpaint = False\n",
        "    if Inpainting:\n",
        "        if Canny or Depth_Map or Open_Pose:\n",
        "            raise TypeError(\"You checked both ControlNet and Inpainting, which will cause incompatibility issues during your run. As of now, there's no alternative way to merge StableDiffusionXLControlNetPipeline and StableDiffusionXLInpaintingPipeline without causing any issues. Perhaps you want to use only one of them?\")\n",
        "        if not Mask_Image:\n",
        "            raise ValueError(\"You checked Inpainting while you're leaving Mask_Image empty. Mask_Image is required for Inpainting!\")\n",
        "        if Inpainting_Image == \"pre-generated text2image image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'text2img')\n",
        "        elif Inpainting_Image == \"pre-generated controlnet image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'controlnet')\n",
        "        elif Inpainting_Image == \"previous inpainting image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'inpaint')\n",
        "        else:\n",
        "            inpaint_image = Inpainting_Image\n",
        "        if inpaint_img is not None and os.path.exists(inpaint_img):\n",
        "            pipeline_type = \"inpaint\"\n",
        "            inpaint_image = load_image(inpaint_img).resize((1024, 1024))\n",
        "            mask_image = load_image(Mask_Image).resize((1024, 1024))\n",
        "            active_inpaint = True\n",
        "            display(make_image_grid([inpaint_image, mask_image], rows=1, cols=2))\n",
        "        else:\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "    if not IP_Image_Link and IP_Adapter != \"None\":\n",
        "        raise ValueError(f\"You selected {IP_Adapter}, but left the IP_Image_Link empty. Please change the IP_Adapter to None or add at least one image in IP_Image_Link!\")\n",
        "    if not Canny_link and not Depthmap_Link and not Openpose_Link and not active_inpaint:\n",
        "        print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "        pipeline_type = \"text2img\"\n",
        "\n",
        "    # Saving parameters config 1st phase\n",
        "    params = [\n",
        "        prompt_widget.value,\n",
        "        model_widget.value,\n",
        "        model_format_widget.value,\n",
        "        negative_prompt_widget.value,\n",
        "        width_slider.value,\n",
        "        height_slider.value,\n",
        "        scheduler_dropdown.value,\n",
        "        steps_slider.value,\n",
        "        scale_slider.value,\n",
        "        vae_link_widget.value,\n",
        "        clip_skip_slider.value,\n",
        "        lora_urls_widget.value,\n",
        "        weight_scale_widget.value,\n",
        "        canny_min_slider.value,\n",
        "        canny_max_slider.value,\n",
        "        canny_link_widget.value,\n",
        "        canny_toggle.value,\n",
        "        canny_strength_slider.value,\n",
        "        depth_map_link_widget.value,\n",
        "        depth_map_toggle.value,\n",
        "        depth_strength_slider.value,\n",
        "        openpose_link_widget.value,\n",
        "        openpose_toggle.value,\n",
        "        openpose_strength_slider.value,\n",
        "        inpainting_image_dropdown.value,\n",
        "        mask_image_widget.value,\n",
        "        inpainting_toggle.value,\n",
        "        inpainting_strength_slider.value,\n",
        "        ip_adapter_dropdown.value,\n",
        "        ip_image_link_widget.value,\n",
        "        ip_adapter_strength_slider.value,\n",
        "        freeze_widget.value,\n",
        "        karras_bool.value,\n",
        "        vpred_bool.value,\n",
        "        sgmuniform_bool.value,\n",
        "        res_betas_zero_snr.value,\n",
        "        vae_config.value\n",
        "    ]\n",
        "    save_param(f\"{base_path}/parameters.json\", params)\n",
        "\n",
        "    # Checking if previous loaded model or pipeline is the same as the new one\n",
        "    global loaded_model, loaded_pipeline\n",
        "    if loaded_model and loaded_model != model_widget.value:\n",
        "        restart(model_widget.value, loaded_model.value)\n",
        "    if loaded_pipeline and loaded_pipeline != pipeline_type:\n",
        "        restart(pipeline_type, loaded_pipeline.value)\n",
        "\n",
        "    # Logic to handle ControlNet and/or MultiControlNets\n",
        "    if Canny and Canny_link is not None:\n",
        "        if \"canny\" not in loaded_controlnet_model:\n",
        "          global canny_model\n",
        "          canny_model = ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True, low_cpu_mem_usage=True)\n",
        "          loaded_controlnet_model[0] = \"canny\"\n",
        "          controlnets[0] = canny_model\n",
        "        print(\"🏞️ | Converting image with Canny Edge Detection...\")\n",
        "        c_img = load_image(Canny_link)\n",
        "        image_canny = np.array(c_img)\n",
        "        image_canny = cv2.Canny(image_canny, minimum_canny_threshold, maximum_canny_threshold)\n",
        "        image_canny = image_canny[:, :, None]\n",
        "        image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
        "        canny_image = ImagePIL.fromarray(image_canny)\n",
        "        print(\"✅ | Canny Edge Detection is complete.\")\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([c_img, canny_image.resize((1024, 1024))], rows=1, cols=2))\n",
        "        images[0] = canny_image.resize((1024, 1024))\n",
        "        controlnets_scale[0] = Canny_Strength\n",
        "\n",
        "    if Depth_Map and Depthmap_Link is not None:\n",
        "        if \"depth\" not in loaded_controlnet_model:\n",
        "          global depthmap_model\n",
        "          loaded_controlnet_model[1] = \"depth\"\n",
        "          depthmap_model = ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True, low_cpu_mem_usage=True).to(\"cuda\")\n",
        "          controlnets[1] = depthmap_model\n",
        "        print(\"🏞️ | Converting image with Depth Map...\")\n",
        "        image_depth = load_image(Depthmap_Link).resize((1024, 1024))\n",
        "        depth_estimator = pipe(\"depth-estimation\")\n",
        "        depth_map = get_depth_map(image_depth, depth_estimator).unsqueeze(0).half().to(\"cpu\")\n",
        "        images[1] = depth_map\n",
        "        depth_map_display = ImagePIL.fromarray(get_depth_map_display(image_depth, depth_estimator))\n",
        "        print(\"✅ | Depth Map is complete.\")\n",
        "        controlnets_scale[1] = Depth_Strength\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([image_depth, depth_map_display], rows=1, cols=2))\n",
        "\n",
        "    if Open_Pose and Openpose_Link is not None:\n",
        "        if \"openpose\" not in loaded_controlnet_model:\n",
        "          global openpose, openpose_model\n",
        "          loaded_controlnet_model[2] = \"openpose\"\n",
        "          openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\").to(\"cpu\")\n",
        "          openpose_model = ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\"cuda\")\n",
        "          controlnets[2] = openpose_model\n",
        "        print(\"🏞️ | Converting image with Open Pose...\")\n",
        "        image_openpose = load_image(Openpose_Link)\n",
        "        openpose_image = openpose(image_openpose)\n",
        "        images[2] = openpose_image.resize((1024, 1024))\n",
        "        print(\"✅ | Open Pose is done.\")\n",
        "        controlnets_scale[2] = Open_Pose_Strength\n",
        "        display(make_image_grid([image_openpose, openpose_image.resize((1024, 1024))], rows=1, cols=2))\n",
        "\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        \"h94/IP-Adapter\",\n",
        "        subfolder=\"models/image_encoder\",\n",
        "        torch_dtype=torch.float16,\n",
        "    ) if IP_Adapter != \"None\" else None\n",
        "\n",
        "    # Logic to handle VAE\n",
        "    if VAE_Link and VAE_Config:\n",
        "        if not os.path.exists(\"/content/VAE\"):\n",
        "            os.mkdir(\"VAE\")\n",
        "        vae_filename = VAE_Link.replace(\"/\", \"_\").replace(\".\", \"_\") + \".safetensors\"\n",
        "        if VAE_Link.startswith(\"http\"):\n",
        "            if \"civitai.com\" in VAE_Link:\n",
        "                if \"?\" in VAE_Link or \"&\" in VAE_Link:\n",
        "                    vae_link = VAE_Link + \"&token=\" + Token\n",
        "                else:\n",
        "                    vae_link = VAE_Link + \"token=\" + Token\n",
        "            else:\n",
        "                vae_link = VAE_Link\n",
        "            if not os.path.exists(f\"/content/VAE/{vae_filename}\"):\n",
        "                !cd /content/VAE; wget -O \"$vae_filename\" \"$vae_link\"\n",
        "                !cd /content/VAE; wget -N \"$VAE_Config\"\n",
        "        vae_path = f\"/content/VAE/{vae_filename}\" if VAE_Link.startswith(\"http\") else VAE_Link\n",
        "        vae = AutoencoderKL.from_single_file(vae_path, config=\"/content/VAE/config.json\", torch_dtype=torch.float16, local_files_only=True)\n",
        "    elif VAE_Link and not VAE_Config:\n",
        "        vae = None\n",
        "        print(\"You inputted a VAE link, but not the config. Config is essential to load the model.\")\n",
        "        print(\"Skipping VAE...\")\n",
        "\n",
        "    # Logic to differentiate if the model is Hugging Face's repository\n",
        "    global pipeline\n",
        "    if Model.count(\"/\") == 1 and (not Model.startswith(\"https://\") or not Model.startswith(\"http://\")):\n",
        "        if not controlnets and not active_inpaint and (pipeline_type != \"text2img\" or not loaded_pipeline) and (not loaded_model or model_widget.value != loaded_model):\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif active_inpaint and not controlnets and (pipeline_type != \"inpaint\" or not loaded_pipeline) and (not loaded_model or model_widget.value != loaded_model):\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif (pipeline_type != \"controlnet\" or not loaded_pipeline) and (not loaded_model or model_widget.value != loaded_model):\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=[element for element in controlnets if element], vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=[element for element in controlnets if element], torch_dtype=torch.float16).to(\"cuda\")\n",
        "    else:\n",
        "        if not os.path.exists(\"/content/Checkpoint\"):\n",
        "            os.mkdir(\"Checkpoint\")\n",
        "        if \".ckpt\" in Model_Format:\n",
        "            format = \".ckpt\"\n",
        "        elif \".safetensors\" in Model_Format:\n",
        "            format = \".safetensors\"\n",
        "        checkpoint_name = f\"checkpoint_model{format}\"\n",
        "        if Token and \"civitai.com\" in Model:\n",
        "            if \"?\" in Model or \"&\" in Model:\n",
        "                checkpoint_link = f\"{Model}&token={Token}\"\n",
        "            else:\n",
        "                checkpoint_link = f\"{Model}token={Token}\"\n",
        "        else:\n",
        "            checkpoint_link = Model\n",
        "        Model_folder = Model.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "        Model_path = f\"/content/Checkpoint/{Model_folder}/{checkpoint_name}\"\n",
        "        Model_path_folder = f\"/content/Checkpoint/{Model_folder}\"\n",
        "        if not os.path.exists(Model_path):\n",
        "            if not os.path.exists(Model_path_folder):\n",
        "                os.mkdir(Model_path_folder)\n",
        "            !cd \"$Model_path_folder\"; wget -O \"$checkpoint_name\" \"$checkpoint_link\"\n",
        "        try:\n",
        "            if not controlnets and not active_inpaint and (pipeline_type != \"text2img\" or not loaded_pipeline) and (not loaded_model or model_widget.value != loaded_model):\n",
        "                if VAE_Link:\n",
        "                    pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                else:\n",
        "                    pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            elif active_inpaint and not controlnets and (pipeline_type != \"inpaint\" or not loaded_pipeline) and (not loaded_model or model_widget.value != loaded_model):\n",
        "                if VAE_Link:\n",
        "                    pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                else:\n",
        "                    pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            elif (pipeline_type != \"controlnet\" or not loaded_pipeline) and (not loaded_model or model_widget.value != loaded_model):\n",
        "                if VAE_Link:\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=[element for element in controlnets if element], vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                else:\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=[element for element in controlnets if element], torch_dtype=torch.float16).to(\"cuda\")\n",
        "        except (ValueError, OSError):\n",
        "            pass\n",
        "            os.remove(Model_path)\n",
        "            if not Token and \"civitai.com\" in Model:\n",
        "                Warning = \"You inputted a CivitAI's link, but your token is empty. It's possible that you got unauthorized access during the download.\"\n",
        "            else:\n",
        "                Warning = \"Did you input the correct link? Or did you use the correct format?\"\n",
        "            raise TypeError(f\"The link ({Model}) contains unsupported file or the download was corrupted. {Warning}\")\n",
        "\n",
        "    if not loaded_model:\n",
        "        loaded_model = model_widget.value\n",
        "    if not loaded_pipeline:\n",
        "        loaded_pipeline = pipeline_type\n",
        "\n",
        "    # Seed, safety checker, and memory attention (Xformers)\n",
        "    pipeline.enable_xformers_memory_efficient_attention()\n",
        "    generator = torch.Generator(\"cpu\").manual_seed(saved_number)\n",
        "    pipeline.safety_checker = None\n",
        "\n",
        "    # Handling schedulers\n",
        "    Prediction_type = \"v_prediction\" if V_Prediction else \"epsilon\"\n",
        "    scheduler_args = {\"prediction_type\": Prediction_type,\n",
        "                           \"use_karras_sigmas\": Karras,\n",
        "                           \"rescale_betas_zero_snr\": Rescale_betas_to_zero_SNR\n",
        "                           }\n",
        "    if SGMUniform:\n",
        "      scheduler_args[\"timestep_spacing\"] = \"trailing\"\n",
        "    Scheduler_used = [\"\", f\"{Scheduler} \", \"\", \"\", \"\"]\n",
        "    Scheduler_used[0] = \"V-Prediction \" if Prediction_type == \"v_prediction\" else \"\"\n",
        "    Scheduler_used[2] = \"Karras \" if Karras else \"\"\n",
        "    Scheduler_used[3] = \"SGMUniform \" if SGMUniform else \"\"\n",
        "    Scheduler_used[4] = \"with zero SNR betas rescaling\" if Rescale_betas_to_zero_SNR else \"\"\n",
        "    if Scheduler == \"DPM++ 2M\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DPM++ 2M SDE\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\", **scheduler_args)\n",
        "    elif Scheduler == \"DPM++ SDE\":\n",
        "        pipeline.scheduler = DPMSolverSinglestepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DPM2\":\n",
        "        pipeline.scheduler = KDPM2DiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DPM2 a\":\n",
        "        pipeline.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DDPM\":\n",
        "        pipeline.scheduler = DDPMScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"Euler\":\n",
        "        pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"Euler a\":\n",
        "        pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"Heun\":\n",
        "        pipeline.scheduler = HeunDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"LMS\":\n",
        "        pipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DEISMultistep\":\n",
        "        pipeline.scheduler = DEISMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"UniPCMultistep\":\n",
        "        pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DDIM\":\n",
        "        pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"PNDM\":\n",
        "        pipeline.scheduler = PNDMScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "\n",
        "    # Prompt weighting using Compel\n",
        "    compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True], truncate_long_prompts=False)\n",
        "    conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "\n",
        "    # Logic to load LoRA(s)\n",
        "    if LoRA_URLs:\n",
        "        # Preprocessing the urls and weight before downloading\n",
        "        lora_list = []\n",
        "        lora_path = []\n",
        "        unique_lora_urls = []\n",
        "        lora_links = [word for word in re.split(r\"\\s*,\\s*\", LoRA_URLs) if word]\n",
        "        if not os.path.exists(\"/content/LoRAs\"):\n",
        "            os.mkdir(\"LoRAs\")\n",
        "        if not Weight_Scale:\n",
        "            scales_string = [\"1\"] * len(lora_links)\n",
        "        elif Weight_Scale and len(re.split(r\",| ,\", Weight_Scale)) < len(lora_links):\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "            for j in range(len(lora_links) - len(scales_string)):\n",
        "                scales_string.append(\"1\")\n",
        "        else:\n",
        "            scales_string = re.split(r\"\\s*,\\s*\", Weight_Scale)\n",
        "        scales = [float(num) for num in scales_string if num]\n",
        "\n",
        "        # Downloading the files and removing any duplicates\n",
        "        for i, link in enumerate(lora_links, start=1):\n",
        "          if link not in unique_lora_urls:\n",
        "            unique_lora_urls.append(link)\n",
        "            path_file = os.path.join(\"/content/LoRAs\", link.replace(\"/\", \"_\").replace(\".\", \"_\"))\n",
        "            if not os.path.exists(path_file) and \"http\" in link:\n",
        "                os.makedirs(path_file)\n",
        "            lora_name = link.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "            lora_file_name = f\"lora_{lora_name}.safetensors\"\n",
        "            if \"civitai.com\" in link and Token:\n",
        "                if \"&\" in link or \"?\" in link:\n",
        "                    civit_link = f\"{link}&token={Token}\"\n",
        "                else:\n",
        "                    civit_link = f\"{link}?token={Token}\"\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$civit_link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            elif not link.startswith(\"/content\"):\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            else:\n",
        "                if link.startswith(\"/content/gdrive/MyDrive\"):\n",
        "                    constructed_gdrive_link = link\n",
        "                else:\n",
        "                    constructed_gdrive_link = f\"/content/gdrive/MyDrive/{link}\"\n",
        "                link_from_gdrive = constructed_gdrive_link.split(\"/\")\n",
        "                lora_path.append(\"/\".join([word for word in link_from_gdrive if \".safetensors\" not in word]))\n",
        "                lora_list.append(link_from_gdrive[-1])\n",
        "\n",
        "        # Loading the downloaded weights into the model\n",
        "        lora_weights = [word for word in lora_list if word.endswith(\".safetensors\")]\n",
        "        lora_names = [word.replace(\".safetensors\", \"\") for word in lora_weights]\n",
        "        for p in range(len(lora_weights)):\n",
        "            try:\n",
        "                pipeline.load_lora_weights(f\"{lora_path[p]}/{lora_weights[p]}\", adapter_name=lora_names[p])\n",
        "            except (ValueError):\n",
        "                print(f\"Skipping {lora_weights[p]}...\")\n",
        "        pipeline.set_adapters(lora_names, adapter_weights=scales)\n",
        "        print(\"LoRAs:\")\n",
        "        for lora in lora_weights:\n",
        "            print(lora)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Logic to handle image(s) for IP-Adapter + display\n",
        "    if IP_Adapter != \"None\":\n",
        "        # Loading the images\n",
        "        adapter_image = []\n",
        "        simple_Url = [word for word in re.split(r\"\\s*,\\s*\", IP_Image_Link) if word]\n",
        "        for link in simple_Url:\n",
        "            adapter_image.append(load_image(link))\n",
        "\n",
        "        # Creating the display\n",
        "        adapter_display = [element for element in adapter_image]\n",
        "        if len(adapter_image) % 3 == 0:\n",
        "            row = len(adapter_image)/3\n",
        "        else:\n",
        "            row = int(len(adapter_image)/3) + 1\n",
        "            for i in range(3*row - len(adapter_image)):\n",
        "                adapter_display.append(load_image(\"https://huggingface.co/IDK-ab0ut/BFIDIW9W29NFJSKAOAOXDOKERJ29W/resolve/main/placeholder.png\"))\n",
        "        print(\"Image(s) for IP-Adapter:\")\n",
        "        display(make_image_grid([element.resize((1024, 1024)) for element in adapter_display], rows=row, cols=3))\n",
        "\n",
        "        # Loading the images to the IP-Adapter\n",
        "        image_embeds = [adapter_image]\n",
        "        pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=IP_Adapter)\n",
        "        pipeline.set_ip_adapter_scale(IP_Adapter_Strength)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate\n",
        "    if not controlnets and not active_inpaint: # For Text2Img\n",
        "        image_save = \"[Text-to-Image]\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "        else:\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                ip_adapter_image=image_embeds,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    elif active_inpaint and not controlnets: # For Inpainting\n",
        "        image_save = \"[Inpainting]\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                image=inpaint_image,\n",
        "                mask_image=mask_image,\n",
        "                generator=generator,\n",
        "                strength=Inpainting_Strength,\n",
        "            ).images[0]\n",
        "        else:\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                ip_adapter_image=image_embeds,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "                image=inpaint_image,\n",
        "                mask_image=mask_image,\n",
        "                strength=Inpainting_Strength,\n",
        "            ).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    else: # For ControlNet\n",
        "        image_save = \"[ControlNet]\"\n",
        "        if Inpainting: # Deprecated. Will raise an error if both ControlNet and Inpainting collide.\n",
        "            '''\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                ).images[0]\n",
        "            '''\n",
        "        else:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=[element for element in images if element is not None],\n",
        "                    controlnet_conditioning_scale=[element for element in controlnets_scale if element],\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    generator=generator,\n",
        "                    image=[element for element in images if element is not None],\n",
        "                    controlnet_conditioning_scale=[element for element in controlnets_scale if element]\n",
        "                ).images[0]\n",
        "\n",
        "    # Saving the image\n",
        "    current_time = time.localtime()\n",
        "    formatted_time = time.strftime(\"[%H-%M-%S %B %d, %Y]\", current_time)\n",
        "    if Save_and_Connect_To_GDrive:\n",
        "        if image_save == \"[Text-to-Image]\":\n",
        "            image_save_path = \"/content/gdrive/MyDrive/Text2Img\"\n",
        "        elif image_save == \"[ControlNet]\":\n",
        "            image_save_path = \"/content/gdrive/MyDrive/ControlNet\"\n",
        "        else:\n",
        "            image_save_path = \"/content/gdrive/MyDrive/Inpainting\"\n",
        "    else:\n",
        "        if image_save == \"[Text-to-Image]\":\n",
        "            image_save_path = \"/content/Text2Img\"\n",
        "        elif image_save == \"[ControlNet]\":\n",
        "            image_save_path = \"/content/ControlNet\"\n",
        "        else:\n",
        "            image_save_path = \"/content/Inpainting\"\n",
        "    if not os.path.exists(image_save_path):\n",
        "            os.makedirs(image_save_path)\n",
        "    split_prompt = re.split(\"\\s*,\\s*\", Prompt.replace(\"<\", \"\").replace(\">\", \"\").replace(\":\", \"_\").replace(\";\", \"_\"))\n",
        "    prompt_name = \" \".join(split_prompt)\n",
        "    generated_image_raw_filename = f\"{image_save} {formatted_time} {prompt_name}\"\n",
        "    generated_image_filename = generated_image_raw_filename[:251] if len(generated_image_raw_filename) > 255 else generated_image_raw_filename\n",
        "    generated_image_savefile = f\"{image_save_path}/{generated_image_filename}.png\"\n",
        "    image.save(generated_image_savefile)\n",
        "    widget()\n",
        "\n",
        "    # Saving parameters config 2nd phase\n",
        "    params = [\n",
        "        prompt_widget.value,\n",
        "        model_widget.value,\n",
        "        model_format_widget.value,\n",
        "        negative_prompt_widget.value,\n",
        "        width_slider.value,\n",
        "        height_slider.value,\n",
        "        scheduler_dropdown.value,\n",
        "        steps_slider.value,\n",
        "        scale_slider.value,\n",
        "        vae_link_widget.value,\n",
        "        clip_skip_slider.value,\n",
        "        lora_urls_widget.value,\n",
        "        weight_scale_widget.value,\n",
        "        canny_min_slider.value,\n",
        "        canny_max_slider.value,\n",
        "        canny_link_widget.value,\n",
        "        canny_toggle.value,\n",
        "        canny_strength_slider.value,\n",
        "        depth_map_link_widget.value,\n",
        "        depth_map_toggle.value,\n",
        "        depth_strength_slider.value,\n",
        "        openpose_link_widget.value,\n",
        "        openpose_toggle.value,\n",
        "        openpose_strength_slider.value,\n",
        "        inpainting_image_dropdown.value,\n",
        "        mask_image_widget.value,\n",
        "        inpainting_toggle.value,\n",
        "        inpainting_strength_slider.value,\n",
        "        ip_adapter_dropdown.value,\n",
        "        ip_image_link_widget.value,\n",
        "        ip_adapter_strength_slider.value,\n",
        "        freeze_widget.value,\n",
        "        karras_bool.value,\n",
        "        vpred_bool.value,\n",
        "        sgmuniform_bool.value,\n",
        "        res_betas_zero_snr.value,\n",
        "        vae_config.value\n",
        "    ]\n",
        "    save_param(f\"{base_path}/parameters.json\", params)\n",
        "\n",
        "    # Handling last generated image\n",
        "    last_generation_json = os.path.join(base_path, \"last_generation.json\")\n",
        "    save_last(last_generation_json, generated_image_savefile, image_save)\n",
        "\n",
        "    # Displaying the image, seed, and scheduler\n",
        "    display(image)\n",
        "    print(f\"Scheduler: {''.join(Scheduler_used)}\")\n",
        "    print(f\"Seed: {saved_number}\")\n",
        "    print(f\"Image is saved at {generated_image_savefile}.\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Lists to prevent ControlNet models from being loaded twice\n",
        "loaded_controlnet_model = [None] * 3\n",
        "controlnets = [None] * 3\n",
        "images = [None] * 3\n",
        "controlnets_scale = [None] * 3\n",
        "\n",
        "# Assigning the click event for submit_button_widget, loading lora urls from saved parameter, and displaying the ui\n",
        "submit_button_widget.on_click(submit)\n",
        "lora_reader_upon_starting()\n",
        "widget()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMmnx4mwtupU"
      },
      "source": [
        "\n",
        "###<font color=\"black\"> » <b><font color=\"purple\">Information </b>✏️📄</font> <font color=\"black\"> «\n",
        "#####ㅤ\n",
        "<small>• Text2Img image is saved in Text2Img folder. </small>\n",
        "\n",
        "<small>• ControlNet-generated image is saved in ControlNet folder. (requires **Canny**, **Depth Map**, and/or **Open Pose** to be checked, as well as the direct link to the reference image) </small>\n",
        "\n",
        "<small>• Inpainting-generated image is saved in Inpainting folder. (requires **Inpainting** to be checked, as well as inputting the image and the mask image)</small>\n",
        "\n",
        "<small> • You can't combine Inpainting and ControlNet.</small>\n",
        "\n",
        "<small>• IP-Adapter doesn't change the image name.</small>\n",
        "\n",
        "<small>• You can load LoRAs from your Google Drive by inputting their path. As of now, only LoRAs are supported. </small>\n",
        "\n",
        "<small>• For ControlNet, leave the image link blank to use the last generated Text2Img image as the reference. Input \"inpaint\" to use the last generated Inpainting image. And lastly, input \"controlnet\" to use the last generated ControlNet image. (requires **Canny**, **Depth Map**, **Inpainting**, and/or **Open Pose** to be checked) </small>\n",
        "\n",
        "***\n",
        "\n",
        "###<font color=\"black\"> » <b><font color=\"cyan\">Guide </b>🚶🏻📋</font> <font color=\"black\"> «\n",
        "#####ㅤ\n",
        "\n",
        "<small> **Prompt:** Basically, this one tells the AI what do you want to see in the image. Sometimes, you have to be strict with your words to align the image with your imagination.\n",
        "\n",
        "<small> **Model (**checkpoint**):** A saved state during an intense training. This is required to generate the image. The type of model you inputted affects the overall style.\n",
        "\n",
        "<small> **Model Format:** This is pretty self-explanatory. If you want to use .safetensors model, then set it to \"Safe Tensors.\"\n",
        "\n",
        "<small> **Steps:** It's simply how many iterations the AI will do in order to generate the image. More doesn't always better. You can look for references online.\n",
        "\n",
        "<small> **Scale (**Guidance Scale**):** This affects how closely related the image with the prompt. High value can be precise, but low value can add extra uniqueness.\n",
        "\n",
        "<small> **VAE:** Stands for Variational Autoencoder. It basically controls the color of your image.\n",
        "\n",
        "<small> **Clip Skip:** Lets the AI skip set amount of layers during generation.\n",
        "\n",
        "<small> **LoRA:** Stands for Low-Rank Adaptation. It holds weight to be \"fed\" to the AI. In simple terms, LoRA guides the AI to draw specific characters, style, poses, and so much more. You also need to specify the LoRA's scale, similar to **Scale**.\n",
        "\n",
        "<small> **ControlNet:** Basically a strict instruction based on the inputted image to generate image closely related to the reference.\n",
        "\n",
        "<small> **Inpainting:** Redrawing an image, but with certain parts of the image changed, just like editing with Photoshop, but AI does the job for you.\n",
        "\n",
        "<small> **IP-Adapter:** Similar to LoRA, but only follows the inputted image(s). This is stricter than LoRA and sometimes lacks generalization.\n",
        "\n",
        "<small> **Negative Prompt:** The reverse version of **Prompt**. Instead of telling the AI what do you want, this tells the AI about what do you want to be removed from the image."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "t8hug-0Okf8t"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
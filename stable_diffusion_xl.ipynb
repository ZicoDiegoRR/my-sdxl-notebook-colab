{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicoDiegoRR/my-sdxl-notebook-colab/blob/main/stable_diffusion_xl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###<font color=\"black\"> ¬ª <b><font color=\"purple\">Information </b>‚úèÔ∏èüìÑ</font> <font color=\"black\"> ¬´\n",
        "#####„Ö§\n",
        "<small>‚Ä¢ Text2Img image is saved in Text2Img folder. </small>\n",
        "\n",
        "<small>‚Ä¢ ControlNet-generated image is saved in ControlNet folder. (requires **Canny**, **Depth_Map**, and/or **Open_Pose** to be checked, as well as the direct link to the reference image) </small>\n",
        "\n",
        "<small>‚Ä¢ Inpainting-generated image is saved in Inpainting folder. (requires **Inpainting** to be checked, as well as inputting the image and the mask image)</small>\n",
        "\n",
        "<small> ‚Ä¢ You can't combine Inpainting and ControlNet.</small>\n",
        "\n",
        "<small>‚Ä¢ IP-Adapter doesn't change the image name.</small>\n",
        "\n",
        "<small>‚Ä¢ You can load LoRAs from your Google Drive by inputting their path. As of now, only LoRAs are supported. (requires <b>Save_and_Connect_To_GDrive</b> to be checked)</small>\n",
        "\n",
        "<small>‚Ä¢ For ControlNet, leave the image link blank to use the last generated Text2Img image as the reference. Input \"inpaint\" to use the last generated Inpainting image. And lastly, input \"controlnet\" to use the last generated ControlNet image. (requires **Canny**, **Depth_Map**, **Inpainting**, and/or **Open_Pose** to be checked) </small>\n",
        "\n",
        "***\n",
        "\n",
        "###<font color=\"black\"> ¬ª <b><font color=\"cyan\">Guide </b>üö∂üèªüìã</font> <font color=\"black\"> ¬´\n",
        "#####„Ö§\n",
        "\n",
        "<small> **Prompt:** Basically, this one tells the AI what do you want to see in the image. Sometimes, you have to be strict with your words to align the image with your imagination.\n",
        "\n",
        "<small> **Model (**checkpoint**):** A saved state during an intense training. This is required to generate the image. The type of model you inputted affects the overall style.\n",
        "\n",
        "<small> **Model Format:** This is pretty self-explanatory. If you want to use .safetensors model, then set it to \"Safe Tensors.\"\n",
        "\n",
        "<small> **Steps:** It's simply how many iterations the AI will do in order to generate the image. More doesn't always better. You can look for references online.\n",
        "\n",
        "<small> **Scale (**Guidance Scale**):** This affects how closely related the image with the prompt. High value can be precise, but low value can add extra uniqueness.\n",
        "\n",
        "<small> **VAE:** Stands for Variational Autoencoder. It basically controls the color of your image.\n",
        "\n",
        "<small> **Clip Skip:** Lets the AI skip set amount of layers during generation.\n",
        "\n",
        "<small> **LoRA:** Stands for Low-Rank Adaptation. It holds weight to be \"fed\" to the AI. In simple terms, LoRA guides the AI to draw specific characters, style, poses, and so much more. You also need to specify the LoRA's scale, similar to **Scale**.\n",
        "\n",
        "<small> **ControlNet:** Basically a strict instruction based on the inputted image to generate image closely related to the reference.\n",
        "\n",
        "<small> **Inpainting:** Redrawing an image, but with certain parts of the image changed, just like editing with Photoshop, but AI does the job for you.\n",
        "\n",
        "<small> **IP-Adapter:** Similar to LoRA, but only follows the inputted image(s). This is stricter than LoRA and sometimes lacks generalization.\n",
        "\n",
        "<small> **Negative Prompt:** The reverse version of **Prompt**. Instead of telling the AI what do you want, this tells the AI about what do you want to be removed from the image.\n",
        "\n",
        "#####‚¨áÔ∏èFor more information, see the cell below‚¨áÔ∏è"
      ],
      "metadata": {
        "id": "xMmnx4mwtupU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hug-0Okf8t"
      },
      "source": [
        "###<font color=\"black\"> ¬ª <b><font color=\"red\">Installing Dependencies </b>üíø</font> <font color=\"black\"> ¬´\n",
        "#####„Ö§Run this cell first before creating images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaGpmeILXSGl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown <b>Run this first to install essential libraries!</b><br>\n",
        "#@markdown <small><p>Required to use the generator.\n",
        "from IPython.display import clear_output\n",
        "print(\"‚öôÔ∏è | Downloading libraries...\")\n",
        "!pip install diffusers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install opencv-python\n",
        "!pip install peft\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install compel\n",
        "!pip install controlnet-aux\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "clear_output()\n",
        "print(\"üìÅ | All essential libraries have been downloaded.\")\n",
        "print(\"üñå | You can start generating images now.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBZ305GvH7w"
      },
      "source": [
        "###<font color=\"black\"> ¬ª <b><font color=\"orange\">MultiControlNet<font color=\"black\">, <b><font color=\"magenta\"></b>IP-Adapter<font color=\"black\">, and <b><font color=\"Lime\">Inpainting</b> üîß</font> <font color=\"black\"> ¬´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sdjCI-xvy5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import ControlNetModel, StableDiffusionXLPipeline, StableDiffusionXLControlNetPipeline, AutoPipelineForInpainting, AutoencoderKL\n",
        "from diffusers import DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler, KDPM2DiscreteScheduler, KDPM2AncestralDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, HeunDiscreteScheduler, LMSDiscreteScheduler, DEISMultistepScheduler, UniPCMultistepScheduler, DDIMScheduler, PNDMScheduler\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline as pipe\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "from google.colab import drive\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import os.path\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Function to save the data to a json\n",
        "def save_last(filename, data, type):\n",
        "    try:\n",
        "        if os.path.exists(filename):\n",
        "            with open(filename, 'r') as file:\n",
        "                existing_data = json.load(file)\n",
        "        else:\n",
        "            existing_data = {}\n",
        "\n",
        "        if type == \"[Text-to-Image]\":\n",
        "            existing_data['text2img'] = data\n",
        "        elif type == \"[ControlNet]\":\n",
        "            existing_data['controlnet'] = data\n",
        "        elif type == \"[Inpainting]\":\n",
        "            existing_data['inpaint'] = data\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(existing_data, file, indent=4)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Function to load last-generated image\n",
        "def load_last(filename, type):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data.get(type, None)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return None\n",
        "\n",
        "# Function to load the saved data from a json\n",
        "def load_number(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data['saved']\n",
        "    except (FileNotFoundError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Function to save the data to a json\n",
        "def save_number(filename, data):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump({'saved': data}, file)\n",
        "\n",
        "# Function to convert image into depth map\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "\n",
        "# Only for display in output, nothing crazy\n",
        "def get_depth_map_display(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    return image\n",
        "# Main function to handle the logic\n",
        "if __name__ == \"__main__\":\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    #@markdown <b>Miscellaneous ‚û°Ô∏èüíΩ</b>\n",
        "    Freeze = False #@param {type:\"boolean\"}\n",
        "    T4_Mode = False #@param {type:\"boolean\"}\n",
        "    Save_and_Connect_To_GDrive = False #@param {type:\"boolean\"}\n",
        "    #@markdown <small><b>Freeze:</b> Using the same seed to generate similar images.</small>\n",
        "\n",
        "    #@markdown <small><b>T4_Mode:</b> Automatically restarts the runtime to unload the GPU's workload to prevent GPU out of memory error. (recommended for T4 GPU)</small>\n",
        "\n",
        "    # Handling Google Drive and seed\n",
        "    folder = \"/content/gdrive/MyDrive/\" if Save_and_Connect_To_GDrive else \"/content/\"\n",
        "    filename = os.path.join(folder, \"random_number.json\")\n",
        "    if Save_and_Connect_To_GDrive:\n",
        "        print(\"üì• | Connecting to Google Drive...\")\n",
        "        drive.mount('/content/gdrive', force_remount=True)\n",
        "    saved_number = load_number(filename)\n",
        "    if not Freeze:\n",
        "        # Generate a new random number if Freeze is False\n",
        "        random_number = random.randint(1, 1000000000)\n",
        "        save_number(filename, random_number)\n",
        "        saved_number = load_number(filename)\n",
        "    else:\n",
        "        # Use the saved number if Freeze is True\n",
        "        if saved_number is not None:\n",
        "            saved_number = saved_number\n",
        "        else:\n",
        "            print(\"No saved seed found. Generating new one...\")\n",
        "            random_number = random.randint(1, 1000000000)\n",
        "            save_number(filename, random_number)\n",
        "            saved_number = load_number(filename)\n",
        "\n",
        "    # Handling user's input‚¨áÔ∏è\n",
        "    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Prompt üñå</b><br>\n",
        "    #@markdown <small>What do you want to see in the image?</small><br>\n",
        "    #@markdown <small>Leave everything unchecked and set the IP-Adapter to \"None\" to generate using Text2Image pipeline.</small>\n",
        "    Prompt = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Currently supports HuggingFace and CivitAI models, but also works for any websites. For HuggingFace's model, provide only the repository's ID. For CivitAI and others, provide the direct link to the model. When you're getting 401 error from CivitAI's link, add your token below.</small>\n",
        "    Model = \"\" #@param {type:\"string\"}\n",
        "    Model_Format = \"Safe Tensor (.safetensors)\" #@param [\"Pickle Tensor (.ckpt)\", \"Safe Tensor (.safetensors)\"]\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Settings ‚öôÔ∏è</b><br>\n",
        "    Width = 1024 #@param {type:\"slider\", min: 512, max:1536, step:64}\n",
        "    Height = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
        "    Scheduler = \"Default (defaulting to the model)\" #@param [\"Default (defaulting to the model)\", \"DPM++ 2M\", \"DPM++ 2M Karras\", \"DPM++ 2M SDE\", \"DPM++ 2M SDE Karras\", \"DPM++ SDE\", \"DPM++ SDE Karras\", \"DPM2\", \"DPM2 Karras\", \"DPM2 a\", \"DPM2 a Karras\", \"DDIM\", \"PNDM\", \"Euler\", \"Euler a\", \"Heun\", \"LMS\", \"LMS Karras\", \"DEISMultistep\", \"UniPCMultistep\"]\n",
        "    Steps = 20 #@param {type:\"number\"}\n",
        "    Scale = 6 #@param {type:\"slider\", min:1, max:12, step:0.1}\n",
        "    #@markdown <small> Only accepts one link for Variational Autoencoder. </small>\n",
        "    VAE_Link = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small>Not recommended to go beyond 3 or 4.</small>\n",
        "    #@markdown <small> This will skip how many layers during generation. Setting \"1\" means skipping the last layer. </small>\n",
        "    Clip_Skip = 2 #@param {type:\"slider\", min:0, max:12, step:1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b> LoRA</b> üìÅüñåÔ∏è\n",
        "\n",
        "    #@markdown <small> You can use multiple direct links to the LoRAs using this format: </small>\n",
        "\n",
        "    #@markdown <font color=\"blue\"><u><small><small> <link/to/file.safetensors>, <link/to/file.bin>, ... </small></small></font></u>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space (or not) between comma and the next link. </small>\n",
        "    LoRA_URLs = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Same as the URLs. </small>\n",
        "    #@markdown <small> The first numbers represent the weight's scale for the first LoRA you just inputted. The logic also applies to every LoRA. </small>\n",
        "    Weight_Scale = \"\" #@param {type:\"string\"}\n",
        "    #@markdown <small> Passing CivitAI's token is optional, but required if you're getting 401 Unauthorized error. Do not share your CivitAI's API key with anyone else!</small>\n",
        "    Token = \"\" #@param {type:\"string\"}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>ControlNet</b> üñºÔ∏èüîß\n",
        "\n",
        "    #@markdown <small> Adjust the thresholds based on your needs. Put an image link to the <b> Canny_Link </b> for reference. </small>\n",
        "    minimum_canny_threshold = 100 #@param {type:\"slider\", min:10, max:500, step:5}\n",
        "    maximum_canny_threshold = 240 #@param {type:\"slider\", min:100, max:750, step:5}\n",
        "    Canny_Link = \"\" #@param {type:\"string\"}\n",
        "    Canny = False #@param {type:\"boolean\"}\n",
        "    Canny_Strength = 0.7 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    #@markdown <small> Put an image link to the <b> DepthMap_Link </b> for reference. </small>\n",
        "    DepthMap_Link = \"\" #@param {type:\"string\"}\n",
        "    Depth_Map = False #@param {type:\"boolean\"}\n",
        "    Depth_Strength = 0.7 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    #@markdown <small> Put an image link to the <b> OpenPose_Link </b> for reference. </small>\n",
        "    OpenPose_Link = \"\" #@param {type:\"string\"}\n",
        "    Open_Pose = False #@param {type:\"boolean\"}\n",
        "    Open_Pose_Strength = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Inpainting</b> üñºÔ∏èüñåÔ∏è\n",
        "\n",
        "    #@markdown <small> Set the <b>Inpainting_Image</b> to \"pre-generated text2image image\" for last generated Text2Image image, \"pre-generated controlnet image\" for last ControlNet's generated image, and \"previous inpainting image\" for last inpainted image. Or you can pick image online using its direct link.</small>\n",
        "    Inpainting_Image = \"pre-generated text2image image\" #@param [\"pre-generated text2image image\", \"pre-generated controlnet image\", \"previous inpainting image\"] {allow-input:true}\n",
        "    Mask_Image = \"\" #@param {type:\"string\"}\n",
        "    Inpainting = False #@param {type:\"boolean\"}\n",
        "    Inpainting_Strength = 0.9 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>IP-Adapter</b> üñºÔ∏èüìù\n",
        "\n",
        "    #@markdown <small> You can use multiple direct links to the images using this format: </small>\n",
        "\n",
        "    #@markdown <small><small><small> https://example1.com/.../file.jpg, https://example2.com/.../file.jpg, ... </small></small></small>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space between comma and the next link. </small>\n",
        "    IP_Adapter = \"None\" #@param [\"ip-adapter-plus_sdxl_vit-h.bin\", \"ip-adapter-plus-face_sdxl_vit-h.bin\", \"ip-adapter_sdxl_vit-h.bin\", \"None\"]\n",
        "    IP_Image_Link=\"\" #@param {type:\"string\"}\n",
        "    IP_Adapter_Strength= 1 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Negative Prompt ‚õî</b><br>\n",
        "    #@markdown <small>What you <b>don't</b> want to see in the image? (optional)</small><br>\n",
        "    Negative_Prompt = \"\" #@param {type:\"string\"}\n",
        "    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "\n",
        "    # Selecting image\n",
        "    if Save_and_Connect_To_GDrive:\n",
        "        base_path = \"/content/gdrive/MyDrive\"\n",
        "    else:\n",
        "        base_path = \"/content\"\n",
        "    last_generation_loading = os.path.join(base_path, \"last_generation.json\")\n",
        "    if Canny:\n",
        "        if Canny_Link == \"inpaint\":\n",
        "            Canny_link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif Canny_Link == \"controlnet\":\n",
        "            Canny_link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not Canny_Link:\n",
        "            Canny_link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Canny_link = Canny_Link\n",
        "        if Canny_link is None and not os.path.exists(Canny_link):\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "    if Depth_Map:\n",
        "        if DepthMap_Link == \"inpaint\":\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif DepthMap_Link == \"controlnet\":\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not DepthMap_Link:\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Depthmap_Link = DepthMap_Link\n",
        "        if Depthmap_Link is None and not os.path.exists(Depthmap_Link):\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "    if Open_Pose:\n",
        "        if OpenPose_Link == \"inpaint\":\n",
        "            Openpose_Link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif OpenPose_Link == \"controlnet\":\n",
        "            Openpose_Link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not OpenPose_Link:\n",
        "            Openpose_Link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Openpose_Link = OpenPose_Link\n",
        "        if Openpose_Link is not None and not os.path.exists(Openpose_Link):\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "    active_inpaint = False\n",
        "    if Inpainting:\n",
        "        if Canny or Depth_Map or Open_Pose:\n",
        "            raise TypeError(\"You checked both ControlNet and Inpainting, which will cause incompatibility issues during your run. As of now, there's no alternative way to merge StableDiffusionXLControlNetPipeline and StableDiffusionXLInpaintingPipeline without causing any issues. Perhaps you want to use only one of them?\")\n",
        "        if not Mask_Image:\n",
        "            raise ValueError(\"You checked Inpainting while you're leaving Mask_Image empty. Mask_Image is required for Inpainting!\")\n",
        "        if Inpainting_Image == \"pre-generated text2image image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'text2img')\n",
        "        elif Inpainting_Image == \"pre-generated controlnet image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'controlnet')\n",
        "        elif Inpainting_Image == \"previous inpainting image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'inpaint')\n",
        "        else:\n",
        "            inpaint_image = Inpainting_Image\n",
        "        if inpaint_img is not None and os.path.exists(inpaint_img):\n",
        "            inpaint_image = load_image(inpaint_img).resize((1024, 1024))\n",
        "            mask_image = load_image(Mask_Image).resize((1024, 1024))\n",
        "            active_inpaint = True\n",
        "            display(make_image_grid([inpaint_image, mask_image], rows=1, cols=2))\n",
        "        else:\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "    if not IP_Image_Link and IP_Adapter != \"None\":\n",
        "        raise ValueError(f\"You selected {IP_Adapter}, but left the IP_Image_Link empty. Please change the IP_Adapter to None or add at least one image in IP_Image_Link!\")\n",
        "\n",
        "    # Logic to handle ControlNet and/or MultiControlNets\n",
        "    controlnets = []\n",
        "    images = []\n",
        "    controlnets_scale = []\n",
        "    if Canny and Canny_link is not None and os.path.exists(Canny_link):\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"üèûÔ∏è | Converting image with Canny Edge Detection...\")\n",
        "        c_img = load_image(Canny_link)\n",
        "        image_canny = np.array(c_img)\n",
        "        image_canny = cv2.Canny(image_canny, minimum_canny_threshold, maximum_canny_threshold)\n",
        "        image_canny = image_canny[:, :, None]\n",
        "        image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
        "        canny_image = Image.fromarray(image_canny)\n",
        "        print(\"‚úÖ | Canny Edge Detection is complete.\")\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([c_img, canny_image.resize((1024, 1024))], rows=1, cols=2))\n",
        "        images.append(canny_image.resize((1024, 1024)))\n",
        "        controlnets_scale.append(Canny_Strength)\n",
        "    if Depth_Map and Depthmap_Link is not None:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"üèûÔ∏è | Converting image with Depth Map...\")\n",
        "        image_depth = load_image(Depthmap_Link).resize((1024, 1024))\n",
        "        depth_estimator = pipe(\"depth-estimation\")\n",
        "        depth_map = get_depth_map(image_depth, depth_estimator).unsqueeze(0).half().to(\"cpu\")\n",
        "        images.append(depth_map)\n",
        "        depth_map_display = Image.fromarray(get_depth_map_display(image_depth, depth_estimator))\n",
        "        print(\"‚úÖ | Depth Map is complete.\")\n",
        "        controlnets_scale.append(Depth_Strength)\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([image_depth, depth_map_display], rows=1, cols=2))\n",
        "    if Open_Pose and Openpose_Link is not None:\n",
        "        openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\").to(\"cpu\")\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16).to(\"cuda\"))\n",
        "        print(\"üèûÔ∏è | Converting image with Open Pose...\")\n",
        "        image_openpose = load_image(Openpose_Link)\n",
        "        openpose_image = openpose(image_openpose)\n",
        "        images.append(openpose_image.resize((1024, 1024)))\n",
        "        print(\"‚úÖ | Open Pose is done.\")\n",
        "        controlnets_scale.append(Open_Pose_Strength)\n",
        "        display(make_image_grid([image_openpose, openpose_image.resize((1024, 1024))], rows=1, cols=2))\n",
        "\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        \"h94/IP-Adapter\",\n",
        "        subfolder=\"models/image_encoder\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    # Logic to handle VAE\n",
        "    if VAE_Link:\n",
        "        if not os.path.exists(\"/content/VAE\"):\n",
        "            os.mkdir(\"VAE\")\n",
        "        vae_filename = VAE_Link.replace(\"/\", \"_\").replace(\".\", \"_\") + \".safetensors\"\n",
        "        if VAE_Link.startswith(\"http\"):\n",
        "            if \"civitai.com\" in VAE_Link:\n",
        "                if \"?\" in VAE_Link or \"&\" in VAE_Link:\n",
        "                    vae_link = VAE_Link + \"&token=\" + Token\n",
        "                else:\n",
        "                    vae_link = VAE_Link + \"token=\" + Token\n",
        "            else:\n",
        "                vae_link = VAE_Link\n",
        "            if not os.path.exists(f\"/content/VAE/{vae_filename}\"):\n",
        "                !cd /content/VAE; wget -O \"$vae_filename\" \"$vae_link\"\n",
        "            if not os.path.exists(\"/content/VAE/config.json\"):\n",
        "                !cd /content/VAE; wget -N https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/vae/config.json\n",
        "        vae_path = f\"/content/VAE/{vae_filename}\" if VAE_Link.startswith(\"http\") else VAE_Link\n",
        "        vae = AutoencoderKL.from_single_file(vae_path, torch_dtype=torch.float16, config=\"/content/VAE/config.json\", local_files_only=True)\n",
        "\n",
        "    # Logic to differentiate if the model is Hugging Face's repository\n",
        "    if Model.count(\"/\") == 1:\n",
        "        if not controlnets and not active_inpaint:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif active_inpaint and not controlnets:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        else:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    else:\n",
        "        if not os.path.exists(\"/content/Checkpoint\"):\n",
        "            os.mkdir(\"Checkpoint\")\n",
        "        if \".ckpt\" in Model_Format:\n",
        "            format = \".ckpt\"\n",
        "        elif \".safetensors\" in Model_Format:\n",
        "            format = \".safetensors\"\n",
        "        checkpoint_name = f\"checkpoint_model{format}\"\n",
        "        if Token and \"civitai.com\" in Model:\n",
        "            if \"?\" in Model or \"&\" in Model:\n",
        "                checkpoint_link = f\"{Model}&token={Token}\"\n",
        "            else:\n",
        "                checkpoint_link = f\"{Model}token={Token}\"\n",
        "        else:\n",
        "            checkpoint_link = Model\n",
        "        Model_folder = checkpoint_link.replace(\"/\", \"_\")\n",
        "        Model_path = f\"/content/Checkpoint/{Model_folder}/{checkpoint_name}\"\n",
        "        Model_path_folder = f\"/content/Checkpoint/{Model_folder}\"\n",
        "        if not os.path.exists(Model_path_folder):\n",
        "            os.mkdir(Model_path_folder)\n",
        "            !cd \"$Model_path_folder\"; wget -O \"$checkpoint_name\" \"$checkpoint_link\"\n",
        "        if not controlnets and not active_inpaint:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif active_inpaint and not controlnets:\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        else:\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    pipeline.enable_xformers_memory_efficient_attention()\n",
        "    generator = torch.Generator(\"cpu\").manual_seed(saved_number)\n",
        "    pipeline.safety_checker = None\n",
        "    if Scheduler == \"DPM++ 2M\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"DPM++ 2M Karras\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "    elif Scheduler == \"DPM++ 2M SDE\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\")\n",
        "    elif Scheduler == \"DPM++ 2M SDE Karras\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\", use_karras_sigmas=True)\n",
        "    elif Scheduler == \"DPM++ SDE\":\n",
        "        pipeline.scheduler = DPMSolverSinglestepScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"DPM++ SDE Karras\":\n",
        "        pipeline.scheduler = DPMSolverSinglestepScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"DPM2\":\n",
        "        pipeline.scheduler = KDPM2DiscreteScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"DPM2 Karras\":\n",
        "        pipeline.scheduler = KDPM2DiscreteScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "    elif Scheduler == \"DPM2 a\":\n",
        "        pipeline.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"DPM2 a Karras\":\n",
        "        pipeline.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "    elif Scheduler == \"Euler\":\n",
        "        pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"Euler a\":\n",
        "        pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"Heun\":\n",
        "        pipeline.scheduler = HeunDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"LMS\":\n",
        "        pipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"LMS Karras\":\n",
        "        pipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "    elif Scheduler == \"DEISMultistep\":\n",
        "        pipeline.scheduler = DEISMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"UniPCMultistep\":\n",
        "        pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    elif Scheduler == \"DDIM\":\n",
        "        pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config, rescale_betas_zero_snr=True, timestep_spacing=\"trailing\")\n",
        "    elif Scheduler == \"PNDM\":\n",
        "        pipeline.scheduler = PNDMScheduler.from_config(pipeline.scheduler.config)\n",
        "    compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True], truncate_long_prompts=False)\n",
        "    conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "\n",
        "    # Logic to load LoRA(s)\n",
        "    if LoRA_URLs:\n",
        "        lora_list = []\n",
        "        lora_path = []\n",
        "        lora_links = re.split(r\"\\s*,\\s*\", LoRA_URLs)\n",
        "        if not os.path.exists(\"/content/LoRAs\"):\n",
        "            os.mkdir(\"LoRAs\")\n",
        "        if not Weight_Scale:\n",
        "            scales_string = [\"1\"] * len(lora_links)\n",
        "        elif Weight_Scale and len(re.split(r\",| ,\", Weight_Scale)) < len(lora_links):\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "            for j in range(len(lora_links) - len(scales_string)):\n",
        "                scales_string.append(\"1\")\n",
        "        else:\n",
        "            scales_string = re.split(r\"\\s*,\\s*\", Weight_Scale)\n",
        "        scales = [float(num) for num in scales_string]\n",
        "\n",
        "        for i, link in enumerate(lora_links, start=1):\n",
        "            path_file = os.path.join(\"/content/LoRAs\", link.replace(\"/\", \"_\").replace(\".\", \"_\"))\n",
        "            if not os.path.exists(path_file) and \"http\" in link:\n",
        "                os.makedirs(path_file)\n",
        "            lora_name = link.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "            lora_file_name = f\"lora_{lora_name}.safetensors\"\n",
        "            if \"civitai.com\" in link and Token:\n",
        "                if \"&\" in link or \"?\" in link:\n",
        "                    civit_link = f\"{link}&token={Token}\"\n",
        "                else:\n",
        "                    civit_link = f\"{link}?token={Token}\"\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$civit_link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            elif not link.startswith(\"/content\"):\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            else:\n",
        "                if link.startswith(\"/content/gdrive/MyDrive\"):\n",
        "                    constructed_gdrive_link = link\n",
        "                else:\n",
        "                    constructed_gdrive_link = f\"/content/gdrive/MyDrive/{link}\"\n",
        "                link_from_gdrive = constructed_gdrive_link.split(\"/\")\n",
        "                lora_path.append(\"/\".join([word for word in link_from_gdrive if \".safetensors\" not in word]))\n",
        "                lora_list.append(link_from_gdrive[-1])\n",
        "        lora_weights = [word for word in lora_list if word.endswith(\".safetensors\")]\n",
        "        lora_names = [word.replace(\".safetensors\", \"\") for word in lora_weights]\n",
        "        for p in range(len(lora_weights)):\n",
        "            pipeline.load_lora_weights(f\"{lora_path[p]}/{lora_weights[p]}\", adapter_name=lora_names[p])\n",
        "        pipeline.set_adapters(lora_names, adapter_weights=scales)\n",
        "        print(\"LoRAs:\")\n",
        "        for lora in lora_weights:\n",
        "            print(lora)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Logic to handle image(s) for IP-Adapter + display\n",
        "    if IP_Adapter != \"None\":\n",
        "        adapter_image = []\n",
        "        simple_Url = re.split(r\"\\s*,\\s*\", IP_Image_Link)\n",
        "        for link in simple_Url:\n",
        "            adapter_image.append(load_image(link))\n",
        "        adapter_display = [element for element in adapter_image]\n",
        "        if len(adapter_image) % 3 == 0:\n",
        "            row = len(adapter_image)/3\n",
        "        else:\n",
        "            row = int(len(adapter_image)/3) + 1\n",
        "            for i in range(3*row - len(adapter_image)):\n",
        "                adapter_display.append(load_image(\"https://huggingface.co/IDK-ab0ut/BFIDIW9W29NFJSKAOAOXDOKERJ29W/resolve/main/placeholder.png\"))\n",
        "        print(\"Image(s) for IP-Adapter:\")\n",
        "        display(make_image_grid([element.resize((1024, 1024)) for element in adapter_display], rows=row, cols=3))\n",
        "        image_embeds = [adapter_image]\n",
        "        pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=IP_Adapter)\n",
        "        pipeline.set_ip_adapter_scale(IP_Adapter_Strength)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate\n",
        "    if not controlnets and not active_inpaint: # For Text2Img\n",
        "        image_save = \"[Text-to-Image]\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "        else:\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                ip_adapter_image=image_embeds,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    elif active_inpaint and not controlnets: # For Inpainting\n",
        "        image_save = \"[Inpainting]\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                image=inpaint_image,\n",
        "                mask_image=mask_image,\n",
        "                generator=generator,\n",
        "                strength=Inpainting_Strength,\n",
        "            ).images[0]\n",
        "        else:\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                ip_adapter_image=image_embeds,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "                image=inpaint_image,\n",
        "                mask_image=mask_image,\n",
        "                strength=Inpainting_Strength,\n",
        "            ).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    else: # For ControlNet\n",
        "        image_save = \"[ControlNet]\"\n",
        "        if Inpainting: # Deprecated. Will raise an error if both ControlNet and Inpainting collide.\n",
        "            '''\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                ).images[0]\n",
        "            '''\n",
        "        else:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale\n",
        "                ).images[0]\n",
        "\n",
        "    # Saving the image\n",
        "    current_time = time.localtime()\n",
        "    formatted_time = time.strftime(\"[%H-%M-%S %B %d, %Y]\", current_time)\n",
        "    if Save_and_Connect_To_GDrive:\n",
        "        if image_save == \"[Text-to-Image]\":\n",
        "            image_save_path = \"/content/gdrive/MyDrive/Text2Img\"\n",
        "        elif image_save == \"[ControlNet]\":\n",
        "            image_save_path = \"/content/gdrive/MyDrive/ControlNet\"\n",
        "        else:\n",
        "            image_save_path = \"/content/gdrive/MyDrive/Inpainting\"\n",
        "    else:\n",
        "        if image_save == \"[Text-to-Image]\":\n",
        "            image_save_path = \"/content/Text2Img\"\n",
        "        elif image_save == \"[ControlNet]\":\n",
        "            image_save_path = \"/content/ControlNet\"\n",
        "        else:\n",
        "            image_save_path = \"/content/Inpainting\"\n",
        "    if not os.path.exists(image_save_path):\n",
        "            os.makedirs(image_save_path)\n",
        "    split_prompt = re.split(\"\\s*,\\s*\", Prompt.replace(\"<\", \"\").replace(\">\", \"\"))\n",
        "    prompt_name = \" \".join(split_prompt)\n",
        "    generated_image_filename = f\"{image_save} {formatted_time} {prompt_name}.jpg\"\n",
        "    generated_image_savefile = f\"{image_save_path}/{generated_image_filename}\"\n",
        "    image.save(generated_image_savefile)\n",
        "    print(f\"Image is saved at {generated_image_savefile}.\")\n",
        "\n",
        "    # Handling last generated image\n",
        "    last_generation_json = os.path.join(base_path, \"last_generation.json\")\n",
        "    save_last(last_generation_json, generated_image_savefile, image_save)\n",
        "\n",
        "    # Displaying the image and restarting the runtime if \"T4_Mode\" is true\n",
        "    display(image)\n",
        "    print(f\"Seed: {saved_number}\")\n",
        "    if T4_Mode:\n",
        "        time.sleep(3)\n",
        "        os.kill(os.getpid(), 9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "t8hug-0Okf8t"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}